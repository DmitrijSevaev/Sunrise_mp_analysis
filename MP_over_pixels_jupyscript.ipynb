{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The whole script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: 0000054222.dat\n",
      "Before the processes 1737458797.9101393\n",
      "From The Process 1737458797.9579823\n",
      "From The Process 1737458797.9676855\n",
      "From The Process 1737458797.9822028\n",
      "From The Process 1737458798.0107205\n",
      "From The Process 1737458798.0298026\n",
      "From The Process 1737458798.5469003\n",
      "From The Process 1737458798.5578787\n",
      "From The Process 1737458798.581703\n",
      "From The Process 1737458798.7217958\n",
      "From The Process 1737458799.0385582\n",
      "From The Process 1737458799.1499593\n",
      "From The Process 1737458799.271706\n",
      "From The Process 1737458799.4344413\n",
      "From The Process 1737458799.5139449\n",
      "From The Process 1737458799.575038\n",
      "From The Process 1737458799.7472038\n",
      "From The Process 1737458799.8056705\n",
      "From The Process 1737458799.9410806\n",
      "From The Process 1737458800.2993653\n",
      "From The Process 1737458800.3043957\n",
      "From The Process 1737458800.5507133\n",
      "From The Process 1737458800.5777469\n",
      "From The Process 1737458800.6267283\n",
      "From The Process 1737458800.7753623\n",
      "From The Process 1737458801.0272012\n",
      "Parallel processing of files files (with each writing to its file) finished in: 5.27 s\n",
      "The feather files with the timestamp differences were combined into the 'combined.feather' file in /media/sj/King4TB/LS2_Data/2024.11.11/500t/MP_test_70/delta_ts_data\n"
     ]
    }
   ],
   "source": [
    "\"\"\" #TODO\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import glob\n",
    "import multiprocessing\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from daplis.functions import calc_diff as cd\n",
    "from daplis.functions import utils\n",
    "from daplis.functions.calibrate import load_calibration_data\n",
    "from numpy import ndarray\n",
    "from pyarrow import feather as ft\n",
    "\n",
    "\n",
    "class MpWizard:\n",
    "\n",
    "    # Initialize by passing the input parameters which later will be\n",
    "    # passed into all internal functions\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str = \"\",\n",
    "        pixels: list = [],\n",
    "        daughterboard_number: str = \"\",\n",
    "        motherboard_number: str = \"\",\n",
    "        firmware_version: str = \"\",\n",
    "        timestamps: int = 512,\n",
    "        delta_window: float = 50e3,\n",
    "        include_offset: bool = False,\n",
    "        apply_calibration: bool = True,\n",
    "        apply_mask: bool = True,\n",
    "        absolute_timestamps: bool = False,\n",
    "        number_of_cores: int = 1,\n",
    "    ):\n",
    "        \"\"\"Initialization of the class.\n",
    "\n",
    "        Set all the input parameters for later use with internal class\n",
    "        functions. Additionally, preload the calibration matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str, optional\n",
    "            _description_, by default \"\"\n",
    "        pixels : list, optional\n",
    "            _description_, by default []\n",
    "        daughterboard_number : str, optional\n",
    "            _description_, by default \"\"\n",
    "        motherboard_number : str, optional\n",
    "            _description_, by default \"\"\n",
    "        firmware_version : str, optional\n",
    "            _description_, by default \"\"\n",
    "        timestamps : int, optional\n",
    "            _description_, by default 512\n",
    "        delta_window : float, optional\n",
    "            _description_, by default 50e3\n",
    "        include_offset : bool, optional\n",
    "            _description_, by default False\n",
    "        apply_calibration : bool, optional\n",
    "            _description_, by default True\n",
    "        apply_mask : bool, optional\n",
    "            _description_, by default True\n",
    "        absolute_timestamps : bool, optional\n",
    "            _description_, by default False\n",
    "        number_of_cores : int, optional\n",
    "            _description_, by default 1\n",
    "        \"\"\"\n",
    "\n",
    "        self.path = path\n",
    "        self.pixels = pixels\n",
    "        self.daughterboard_number = daughterboard_number\n",
    "        self.motherboard_number = motherboard_number\n",
    "        self.firmware_version = firmware_version\n",
    "        self.timestamps = timestamps\n",
    "        self.delta_window = delta_window\n",
    "        self.include_offset = include_offset\n",
    "        self.apply_calibration = apply_calibration\n",
    "        self.apply_mask = apply_mask\n",
    "        self.absolute_timestamps = absolute_timestamps\n",
    "        self.number_of_cores = number_of_cores\n",
    "\n",
    "        os.chdir(self.path)\n",
    "\n",
    "        # Load calibration if requested\n",
    "        if self.apply_calibration:\n",
    "\n",
    "            # work_dir = Path(__file__).resolve().parent.parent\n",
    "\n",
    "            # TODO\n",
    "            # path_calibration_data = os.path.join(\n",
    "            #     work_dir, r\"params\\calibration_data\"\n",
    "            # )\n",
    "            path_calibration_data = (\n",
    "                r\"/home/sj/GitHub/daplis/src/daplis/params/calibration_data\"\n",
    "            )\n",
    "            # path_calibration_data = r\"C:\\Users\\bruce\\Documents\\GitHub\\daplis\\src\\daplis\\params\\calibration_data\"\n",
    "\n",
    "            calibration_data = load_calibration_data(\n",
    "                path_calibration_data,\n",
    "                daughterboard_number,\n",
    "                motherboard_number,\n",
    "                firmware_version,\n",
    "                include_offset,\n",
    "            )\n",
    "\n",
    "            if self.include_offset:\n",
    "                self.calibration_matrix, self.offset_array = calibration_data\n",
    "            else:\n",
    "                self.calibration_matrix = calibration_data\n",
    "\n",
    "        # Apply mask if requested\n",
    "        if self.apply_mask:\n",
    "            mask = utils.apply_mask(\n",
    "                self.daughterboard_number,\n",
    "                self.motherboard_number,\n",
    "            )\n",
    "            if isinstance(self.pixels[0], int) and isinstance(\n",
    "                self.pixels[1], int\n",
    "            ):\n",
    "                self.pixels = [pix for pix in self.pixels if pix not in mask]\n",
    "            else:\n",
    "                self.pixels = [\n",
    "                    [value for value in sublist if value not in mask]\n",
    "                    for sublist in pixels\n",
    "                ]\n",
    "\n",
    "        # Check the firmware version and set the pixel coordinates accordingly\n",
    "        if self.firmware_version == \"2212s\":\n",
    "            self.pix_coor = np.arange(256).reshape(4, 64).T\n",
    "        elif firmware_version == \"2212b\":\n",
    "            self.pix_coor = np.arange(256).reshape(64, 4)\n",
    "        else:\n",
    "            print(\"\\nFirmware version is not recognized.\")\n",
    "            sys.exit()\n",
    "\n",
    "    def _unpack_binary_data(\n",
    "        self,\n",
    "        file: str,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Unpack binary data from LinoSPAD2.\n",
    "\n",
    "        Same unpacking function as the standard 'daplis' one, except\n",
    "        the calibration matrix is preloaded during initialization and\n",
    "        called here. Return a 3D matrix of pixel numbers and timestamps.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file : str\n",
    "            A '.dat' data file from LinoSPAD2 with the binary-encoded\n",
    "            data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            A 3D matrix of the pixel numbers where timestamp was\n",
    "            recorded and timestamps themselves.\n",
    "        \"\"\"\n",
    "        # Unpack binary data\n",
    "        raw_data = np.memmap(file, dtype=np.uint32)\n",
    "        # Timestamps are stored in the lower 28 bits\n",
    "        data_timestamps = (raw_data & 0xFFFFFFF).astype(np.int64)\n",
    "        # Pixel address in the given TDC is 2 bits above timestamp\n",
    "        data_pixels = ((raw_data >> 28) & 0x3).astype(np.int8)\n",
    "        # Check the top bit, assign '-1' to invalid timestamps\n",
    "        data_timestamps[raw_data < 0x80000000] = -1\n",
    "\n",
    "        # Number of acquisition cycles in each data file\n",
    "        cycles = len(data_timestamps) // (self.timestamps * 65)\n",
    "        # Transform into a matrix of size 65 by cycles*timestamps\n",
    "        data_pixels = (\n",
    "            data_pixels.reshape(cycles, 65, self.timestamps)\n",
    "            .transpose((1, 0, 2))\n",
    "            .reshape(65, -1)\n",
    "        )\n",
    "\n",
    "        data_timestamps = (\n",
    "            data_timestamps.reshape(cycles, 65, self.timestamps)\n",
    "            .transpose((1, 0, 2))\n",
    "            .reshape(65, -1)\n",
    "        )\n",
    "\n",
    "        # Cut the 65th TDC that does not hold any actual data from pixels\n",
    "        data_pixels = data_pixels[:-1]\n",
    "        data_timestamps = data_timestamps[:-1]\n",
    "\n",
    "        # Insert '-2' at the end of each cycle\n",
    "        insert_indices = np.linspace(\n",
    "            self.timestamps, cycles * self.timestamps, cycles\n",
    "        ).astype(np.int64)\n",
    "\n",
    "        data_pixels = np.insert(\n",
    "            data_pixels,\n",
    "            insert_indices,\n",
    "            -2,\n",
    "            1,\n",
    "        )\n",
    "        data_timestamps = np.insert(\n",
    "            data_timestamps,\n",
    "            insert_indices,\n",
    "            -2,\n",
    "            1,\n",
    "        )\n",
    "\n",
    "        # Combine both matrices into a single one, where each cell holds pixel\n",
    "        # coordinates in the TDC and the timestamp\n",
    "        data_all = np.stack((data_pixels, data_timestamps), axis=2).astype(\n",
    "            np.int64\n",
    "        )\n",
    "\n",
    "        if self.apply_calibration is False:\n",
    "            data_all[:, :, 1] = data_all[:, :, 1] * 2500 / 140\n",
    "        else:\n",
    "            # Path to the calibration data\n",
    "            pix_coordinates = np.arange(256).reshape(64, 4)\n",
    "            for i in range(256):\n",
    "                # Transform pixel number to TDC number and pixel\n",
    "                # coordinates in that TDC (from 0 to 3)\n",
    "                tdc, pix = np.argwhere(pix_coordinates == i)[0]\n",
    "                # Find data from that pixel\n",
    "                ind = np.where(data_all[tdc].T[0] == pix)[0]\n",
    "                # Cut non-valid timestamps ('-1's)\n",
    "                ind = ind[data_all[tdc].T[1][ind] >= 0]\n",
    "                if not np.any(ind):\n",
    "                    continue\n",
    "                data_cut = data_all[tdc].T[1][ind]\n",
    "                # Apply calibration; offset is added due to how delta\n",
    "                # ts are calculated\n",
    "                if self.include_offset:\n",
    "                    data_all[tdc].T[1][ind] = (\n",
    "                        (data_cut - data_cut % 140) * 2500 / 140\n",
    "                        + self.calibration_matrix[i, (data_cut % 140)]\n",
    "                        + self.offset_array[i]\n",
    "                    )\n",
    "                else:\n",
    "                    data_all[tdc].T[1][ind] = (\n",
    "                        data_cut - data_cut % 140\n",
    "                    ) * 2500 / 140 + self.calibration_matrix[\n",
    "                        i, (data_cut % 140)\n",
    "                    ]\n",
    "\n",
    "        return data_all\n",
    "\n",
    "    def _calculate_differences_2212_fast(\n",
    "        self,\n",
    "        data: ndarray,\n",
    "        delta_window: float = 50e3,\n",
    "        cycle_length: float = 4e9,\n",
    "    ):\n",
    "        \"\"\"Calculate timestamp differences for firmware version 2212.\n",
    "\n",
    "        Calculate timestamp differences for the given pixels and LinoSPAD2\n",
    "        firmware version 2212. Modified compared to the standard 'daplis'\n",
    "        one. Modifications are for working with prechunked data, i.e.,\n",
    "        sliced down to two arrays from the whole 64xN matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : ndarray\n",
    "            Matrix of timestamps, where rows correspond to the TDCs.\n",
    "        pixels : List[int] | List[List[int]]\n",
    "            List of pixel numbers for which the timestamp differences should\n",
    "            be calculated or list of two lists with pixel numbers for peak\n",
    "            vs. peak calculations.\n",
    "        pix_coor : ndarray\n",
    "            Array for transforming the pixel address in terms of TDC (0 to 3)\n",
    "            to pixel number in terms of half of the sensor (0 to 255).\n",
    "        delta_window : float, optional\n",
    "            Width of the time window for counting timestamp differences.\n",
    "            The default is 50e3 (50 ns).\n",
    "        cycle_length : float, optional\n",
    "            Length of each acquisition cycle. The default is 4e9 (4 ms).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        deltas_all : dict\n",
    "            Dictionary containing timestamp differences for each pair of pixels.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Dictionary for the timestamp differences, where keys are the\n",
    "        # pixel numbers of the requested pairs\n",
    "\n",
    "        deltas_all = []\n",
    "\n",
    "        data_pix_1 = data[0]\n",
    "        data_pix_2 = data[1]\n",
    "\n",
    "        indices1 = data_pix_1[0]\n",
    "        indices2 = data_pix_2[0]\n",
    "        timestamps1 = data_pix_1[1]\n",
    "        timestamps2 = data_pix_2[1]\n",
    "\n",
    "        timestamps_1 = []\n",
    "        timestamps_2 = []\n",
    "\n",
    "        # Go over cycles, shifting the timestamps from each next\n",
    "        # cycle by lengths of cycles before (e.g., for the 4th cycle\n",
    "        # add 12 ms)\n",
    "        for i, _ in enumerate(self.cycle_ends[:-1]):\n",
    "            slice_from = self.cycle_ends[i]\n",
    "            slice_to = self.cycle_ends[i + 1]\n",
    "            pix1_slice = indices1[\n",
    "                (indices1 >= slice_from) & (indices1 < slice_to)\n",
    "            ]\n",
    "            if not np.any(pix1_slice):\n",
    "                continue\n",
    "            pix2_slice = indices2[\n",
    "                (indices2 >= slice_from) & (indices2 < slice_to)\n",
    "            ]\n",
    "            if not np.any(pix2_slice):\n",
    "                continue\n",
    "\n",
    "            # Shift timestamps by cycle length\n",
    "            tmsp1 = timestamps1[np.isin(indices1, pix1_slice)]\n",
    "            tmsp1 = tmsp1[tmsp1 > 0]\n",
    "            tmsp1 = tmsp1 + cycle_length * i\n",
    "\n",
    "            tmsp2 = timestamps2[np.isin(indices2, pix2_slice)]\n",
    "            tmsp2 = tmsp2[tmsp2 > 0]\n",
    "            tmsp2 = tmsp2 + cycle_length * i\n",
    "\n",
    "            timestamps_1.extend(tmsp1)\n",
    "            timestamps_2.extend(tmsp2)\n",
    "\n",
    "        timestamps_1 = np.array(timestamps_1)\n",
    "        timestamps_2 = np.array(timestamps_2)\n",
    "\n",
    "        # Indicators for each pixel: 0 for timestamps from one pixel\n",
    "        # 1 - from the other\n",
    "        pix1_ind = np.zeros(len(timestamps_1), dtype=np.int32)\n",
    "        pix2_ind = np.ones(len(timestamps_2), dtype=np.int32)\n",
    "\n",
    "        pix1_data = np.vstack((pix1_ind, timestamps_1))\n",
    "        pix2_data = np.vstack((pix2_ind, timestamps_2))\n",
    "\n",
    "        # Dataframe for each pixel with pixel indicator and\n",
    "        # timestamps\n",
    "        df1 = pd.DataFrame(pix1_data.T, columns=[\"Pixel_index\", \"Timestamp\"])\n",
    "        df2 = pd.DataFrame(pix2_data.T, columns=[\"Pixel_index\", \"Timestamp\"])\n",
    "\n",
    "        # Combine the two dataframes\n",
    "        df_combined = pd.concat((df1, df2), ignore_index=True)\n",
    "\n",
    "        # Sort the timestamps\n",
    "        df_combined.sort_values(\"Timestamp\", inplace=True)\n",
    "\n",
    "        # Subtract pixel indicators of neighbors; values of 0\n",
    "        # correspond to timestamp differences for the same pixel\n",
    "        # '-1' and '1' - to differences from different pixels\n",
    "        df_combined[\"Pixel_index_diff\"] = df_combined[\"Pixel_index\"].diff()\n",
    "\n",
    "        # Calculate timestamp difference between neighbors\n",
    "        df_combined[\"Timestamp_diff\"] = df_combined[\"Timestamp\"].diff()\n",
    "\n",
    "        # Get the correct timestamp difference sign\n",
    "        df_combined[\"Timestamp_diff\"] = (\n",
    "            df_combined[\"Timestamp_diff\"] * df_combined[\"Pixel_index_diff\"]\n",
    "        )\n",
    "\n",
    "        # Collect timestamp differences where timestamps are from\n",
    "        # different pixels\n",
    "        filtered_df = df_combined[abs(df_combined[\"Pixel_index_diff\"]) == 1]\n",
    "\n",
    "        # Save only timestamps differences in the requested window\n",
    "        delta_ts = filtered_df[\n",
    "            abs(filtered_df[\"Timestamp_diff\"]) < delta_window\n",
    "        ][\"Timestamp_diff\"].values\n",
    "\n",
    "        deltas_all.extend(delta_ts)\n",
    "\n",
    "        return deltas_all\n",
    "\n",
    "    def _calculate_timestamps_differences(self, args):\n",
    "        \"\"\"Calculate photon coincidences and save to '.feather'.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        #TODO\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"From The Process\", time.time())\n",
    "\n",
    "        # try-except railguard for a function that goes to separate\n",
    "        # cores\n",
    "        file, data, pixel_pair = args\n",
    "        try:\n",
    "            # Check if the 'delta_ts_data' folder exists\n",
    "            output_dir = Path(self.path) / \"delta_ts_data\"\n",
    "            output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "            # Calculate the differences and convert them to a pandas\n",
    "            # dataframe - standard approach sending the whole matrix of\n",
    "            # 64xN to each child process\n",
    "            # deltas_all = cd.calculate_differences_2212_fast(\n",
    "            #     data, pixel_pair, self.pix_coor\n",
    "            # )\n",
    "\n",
    "            # Calculate the differences and convert them to a pandas\n",
    "            # dataframe - pre-chunking the data down to 2 arrays\n",
    "            deltas_all = self._calculate_differences_2212_fast(data)\n",
    "            data_for_plot_df = pd.DataFrame(\n",
    "                deltas_all, columns=[f\"{pixel_pair[0]},{pixel_pair[1]}\"]\n",
    "            ).T\n",
    "\n",
    "            # Save the data to a '.feather' file\n",
    "            file_name = Path(\n",
    "                file\n",
    "            ).stem  # Get the file name without the extension\n",
    "            output_file = (\n",
    "                output_dir\n",
    "                / f\"{file_name}_{pixel_pair[0]}_{pixel_pair[1]}.feather\"\n",
    "            )\n",
    "            file_name = Path(file).stem\n",
    "            output_file = (\n",
    "                output_dir\n",
    "                / f\"{file_name}_{pixel_pair[0]}_{pixel_pair[1]}.feather\"\n",
    "            )\n",
    "            ft.write_feather(\n",
    "                data_for_plot_df.reset_index(drop=True), output_file\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "    def _combine_feather_files(self, path_to_feather_files: str):\n",
    "\n",
    "        os.chdir(path_to_feather_files)\n",
    "\n",
    "        for pixel_pair in self.pixel_pairs:\n",
    "            ft_files = glob.glob(f\"*{pixel_pair[0]}_{pixel_pair[1]}*.feather\")\n",
    "            data_all = pd.DataFrame()\n",
    "            for ft_file in ft_files:\n",
    "                data = ft.read_feather(ft_file)\n",
    "                data_all = pd.concat((data_all, data), ignore_index=True)\n",
    "            data_all.to_feather(\n",
    "                f\"combined_{pixel_pair[0]}_{pixel_pair[1]}.feather\"\n",
    "            )\n",
    "\n",
    "    ### Heavy data chunking - fast calculation in each child process at\n",
    "    ### the cost of long chunking - no acceleration as a result\n",
    "    # def _chunk_that_data_too_much(self, data, pixel_pair, cycle_length: float = 4e9):\n",
    "\n",
    "    #     pix_coor = np.arange(256).reshape(64, 4)\n",
    "\n",
    "    #     # Find ends of cycles\n",
    "    #     cycle_ends = np.argwhere(data[0].T[0] == -2)\n",
    "    #     cycle_ends = np.insert(cycle_ends, 0, 0)\n",
    "\n",
    "    #     tdc1, pix_c1 = np.argwhere(pix_coor == pixel_pair[0])[0]\n",
    "    #     pix1 = np.where(data[tdc1].T[0] == pix_c1)[0]\n",
    "\n",
    "    #     timestamps_1 = []\n",
    "    #     timestamps_2 = []\n",
    "\n",
    "    #     # Second pixel in the pair\n",
    "    #     tdc2, pix_c2 = np.argwhere(pix_coor == pixel_pair[1])[0]\n",
    "    #     pix2 = np.where(data[tdc2].T[0] == pix_c2)[0]\n",
    "\n",
    "    #     # Go over cycles, shifting the timestamps from each next\n",
    "    #     # cycle by lengths of cycles before (e.g., for the 4th cycle\n",
    "    #     # add 12 ms)\n",
    "    #     for i, _ in enumerate(cycle_ends[:-1]):\n",
    "    #         slice_from = cycle_ends[i]\n",
    "    #         slice_to = cycle_ends[i + 1]\n",
    "    #         pix1_slice = pix1[(pix1 >= slice_from) & (pix1 < slice_to)]\n",
    "    #         if not np.any(pix1_slice):\n",
    "    #             continue\n",
    "    #         pix2_slice = pix2[(pix2 >= slice_from) & (pix2 < slice_to)]\n",
    "    #         if not np.any(pix2_slice):\n",
    "    #             continue\n",
    "\n",
    "    #         # Shift timestamps by cycle length\n",
    "    #         tmsp1 = data[tdc1].T[1][pix1_slice]\n",
    "    #         tmsp1 = tmsp1[tmsp1 > 0]\n",
    "    #         tmsp1 = tmsp1 + cycle_length * i\n",
    "\n",
    "    #         tmsp2 = data[tdc2].T[1][pix2_slice]\n",
    "    #         tmsp2 = tmsp2[tmsp2 > 0]\n",
    "    #         tmsp2 = tmsp2 + cycle_length * i\n",
    "\n",
    "    #         timestamps_1.extend(tmsp1)\n",
    "    #         timestamps_2.extend(tmsp2)\n",
    "\n",
    "    #     timestamps_1 = np.array(timestamps_1)\n",
    "    #     timestamps_2 = np.array(timestamps_2)\n",
    "\n",
    "    #     return (timestamps_1, timestamps_2)\n",
    "\n",
    "    def _chunk_that_data(self, data, pixel_pair, cycle_length: float = 4e9):\n",
    "        \"\"\"Chunk data down to 2 rows - prepare for child processes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : np.ndarray\n",
    "            The whole matrix.\n",
    "        pixel_pair : list\n",
    "            Pair of pixels to slice out of the whole matrix.\n",
    "        cycle_length : float, optional\n",
    "            Cycle length in ps, by default 4e9\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            Tuple of two arrays, each contains timestamps from the two\n",
    "            pixels requested and the indices of the timestamps in the\n",
    "            original matrix of data. The indices are used for correct\n",
    "            assigning to the corresponding acquisition cycles.\n",
    "        \"\"\"\n",
    "\n",
    "        tdc1, pix_c1 = np.argwhere(self.pix_coor == pixel_pair[0])[0]\n",
    "        indices1 = np.where(data[tdc1].T[0] == pix_c1)[0]\n",
    "\n",
    "        # Second pixel in the pair\n",
    "        tdc2, pix_c2 = np.argwhere(self.pix_coor == pixel_pair[1])[0]\n",
    "        indices2 = np.where(data[tdc2].T[0] == pix_c2)[0]\n",
    "\n",
    "        data_cut_1 = np.array((indices1, data[tdc1].T[1][indices1]))\n",
    "        data_cut_2 = np.array((indices2, data[tdc2].T[1][indices2]))\n",
    "\n",
    "        return (data_cut_1, data_cut_2)\n",
    "\n",
    "    def calculate_and_save_timestamp_differences_mp(self):\n",
    "\n",
    "        # Find all LinoSPAD2 data files\n",
    "        files = glob.glob(\"*.dat\")\n",
    "\n",
    "        if not files:\n",
    "            raise ValueError(\"No .dat files found in the specified path.\")\n",
    "\n",
    "        self.pixel_pairs = []\n",
    "        for i in self.pixels[0]:\n",
    "            for j in self.pixels[1]:\n",
    "                self.pixel_pairs.append([i, j])\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Go file by file\n",
    "        for file in files:\n",
    "            print(f\"Processing file: {file}\")\n",
    "\n",
    "            start_time_unpack = time.time()\n",
    "\n",
    "            # Unpack the data from the file\n",
    "            data = self._unpack_binary_data(file)\n",
    "\n",
    "            # Pre-collect the indices of the acquisition cycles' ends\n",
    "            self.cycle_ends = np.argwhere(data[0].T[0] == -2)\n",
    "            self.cycle_ends = np.insert(self.cycle_ends, 0, 0)\n",
    "\n",
    "            start_time_args = time.time()\n",
    "\n",
    "            # Prepare the arguments for the child processes:\n",
    "            # 'file' and 'pixel_pair' for naming purposes,\n",
    "            # chunked data as a tuple of two arrays to work with\n",
    "            args = [\n",
    "                (\n",
    "                    file,\n",
    "                    self._chunk_that_data(data, pixel_pair, 250e6),\n",
    "                    pixel_pair,\n",
    "                )\n",
    "                for pixel_pair in self.pixel_pairs\n",
    "            ]\n",
    "\n",
    "            # print(f\"Created args in {time.time() - start_time_args}\")\n",
    "            print(\"Before the processes\", time.time())\n",
    "\n",
    "            with multiprocessing.Pool(\n",
    "                min(self.number_of_cores, len(self.pixel_pairs))\n",
    "            ) as pool:\n",
    "                pool.map(\n",
    "                    self._calculate_timestamps_differences,\n",
    "                    args,\n",
    "                )\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(\n",
    "            f\"Parallel processing of files \"\n",
    "            \"files (with each writing to its file) finished \"\n",
    "            f\"in: {round(end_time - start_time, 2)} s\"\n",
    "        )\n",
    "\n",
    "        # Combine '.feather' files from separate cores\n",
    "        path_to_feathers = os.path.join(self.path, \"delta_ts_data\")\n",
    "\n",
    "        self._combine_feather_files(path_to_feathers)\n",
    "\n",
    "        path_output = os.path.join(self.path, \"delta_ts_data\")\n",
    "\n",
    "        print(\n",
    "            \"The feather files with the timestamp differences were \"\n",
    "            f\"combined into the 'combined.feather' file in {path_output}\"\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # path = r\"D:\\LinoSPAD2\\Data\\B7d\\2024.11.11\\500t\\MP_test\"\n",
    "    path = r'/media/sj/King4TB/LS2_Data/2024.11.11/500t/MP_test_70'\n",
    "\n",
    "    mp = MpWizard(\n",
    "        path,\n",
    "        # pixels=[[144], [171, 172]],\n",
    "        pixels=[[x for x in range(55, 60)], [x for x in range(175, 180)]],\n",
    "        # pixels=[[x for x in range(20, 80)], [x for x in range(130, 190)]],\n",
    "        daughterboard_number=\"B7d\",\n",
    "        motherboard_number=\"#28\",\n",
    "        firmware_version=\"2212b\",\n",
    "        timestamps=500,\n",
    "        number_of_cores=5,\n",
    "    )\n",
    "\n",
    "    mp.calculate_and_save_timestamp_differences_mp()\n",
    "\n",
    "\n",
    "### Standard approach - for control and comparison\n",
    "\n",
    "# import time\n",
    "\n",
    "# from daplis.functions import delta_t\n",
    "\n",
    "# time_start = time.time()\n",
    "\n",
    "# path = r\"/media/sj/King4TB/LS2_Data/2024.11.11/500t/MP_test_70\"\n",
    "\n",
    "\n",
    "# delta_t.calculate_and_save_timestamp_differences_fast(\n",
    "#     path,\n",
    "#     rewrite=True,\n",
    "#     # pixels=[144, 171],\n",
    "#     pixels=[[x for x in range(55, 60)], [x for x in range(175, 180)]],\n",
    "#     # pixels=[[x for x in range(20, 80)], [x for x in range(130, 190)]],\n",
    "#     daughterboard_number=\"B7d\",\n",
    "#     motherboard_number=\"#28\",\n",
    "#     firmware_version=\"2212b\",\n",
    "#     timestamps=500,\n",
    "# )\n",
    "\n",
    "# print(f\"Finished in {time.time() - time_start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First approach: send the whole data array to each child process\n",
    "\n",
    "The slowest so far because the size of the data array is ~400 MB and it presumably takes a lot of time\n",
    "to send a copy of the array to each child process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: 0000054222.dat\n",
      "Before the processes 1737458942.433863\n",
      "From The Process 1737458943.4432573\n",
      "From The Process 1737458943.7281208\n",
      "From The Process 1737458944.2579844\n",
      "From The Process 1737458944.5373874\n",
      "From The Process 1737458945.0620754\n",
      "From The Process 1737458945.3460772\n",
      "From The Process 1737458945.875059\n",
      "From The Process 1737458946.3766\n",
      "From The Process 1737458946.6815412\n",
      "From The Process 1737458947.0119455\n",
      "From The Process 1737458947.4802182\n",
      "From The Process 1737458947.8315656\n",
      "From The Process 1737458948.2624516\n",
      "From The Process 1737458948.6082184\n",
      "From The Process 1737458949.0463061\n",
      "From The Process 1737458949.3998938\n",
      "From The Process 1737458949.8204172\n",
      "From The Process 1737458950.3264365\n",
      "From The Process 1737458950.5878406\n",
      "From The Process 1737458950.9382226\n",
      "From The Process 1737458951.3753674\n",
      "From The Process 1737458951.7286372\n",
      "From The Process 1737458952.1567428\n",
      "From The Process 1737458952.509657\n",
      "From The Process 1737458952.9016325\n",
      "Parallel processing of files files (with each writing to its file) finished in: 12.33 s\n",
      "The feather files with the timestamp differences were combined into the 'combined.feather' file in /media/sj/King4TB/LS2_Data/2024.11.11/500t/MP_test_70/delta_ts_data\n"
     ]
    }
   ],
   "source": [
    "\"\"\" #TODO\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import glob\n",
    "import multiprocessing\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from daplis.functions import calc_diff as cd\n",
    "from daplis.functions import utils\n",
    "from daplis.functions.calibrate import load_calibration_data\n",
    "from numpy import ndarray\n",
    "from pyarrow import feather as ft\n",
    "\n",
    "\n",
    "class MpWizard:\n",
    "\n",
    "    # Initialize by passing the input parameters which later will be\n",
    "    # passed into all internal functions\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str = \"\",\n",
    "        pixels: list = [],\n",
    "        daughterboard_number: str = \"\",\n",
    "        motherboard_number: str = \"\",\n",
    "        firmware_version: str = \"\",\n",
    "        timestamps: int = 512,\n",
    "        delta_window: float = 50e3,\n",
    "        include_offset: bool = False,\n",
    "        apply_calibration: bool = True,\n",
    "        apply_mask: bool = True,\n",
    "        absolute_timestamps: bool = False,\n",
    "        number_of_cores: int = 1,\n",
    "    ):\n",
    "        \"\"\"Initialization of the class.\n",
    "\n",
    "        Set all the input parameters for later use with internal class\n",
    "        functions. Additionally, preload the calibration matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str, optional\n",
    "            _description_, by default \"\"\n",
    "        pixels : list, optional\n",
    "            _description_, by default []\n",
    "        daughterboard_number : str, optional\n",
    "            _description_, by default \"\"\n",
    "        motherboard_number : str, optional\n",
    "            _description_, by default \"\"\n",
    "        firmware_version : str, optional\n",
    "            _description_, by default \"\"\n",
    "        timestamps : int, optional\n",
    "            _description_, by default 512\n",
    "        delta_window : float, optional\n",
    "            _description_, by default 50e3\n",
    "        include_offset : bool, optional\n",
    "            _description_, by default False\n",
    "        apply_calibration : bool, optional\n",
    "            _description_, by default True\n",
    "        apply_mask : bool, optional\n",
    "            _description_, by default True\n",
    "        absolute_timestamps : bool, optional\n",
    "            _description_, by default False\n",
    "        number_of_cores : int, optional\n",
    "            _description_, by default 1\n",
    "        \"\"\"\n",
    "\n",
    "        self.path = path\n",
    "        self.pixels = pixels\n",
    "        self.daughterboard_number = daughterboard_number\n",
    "        self.motherboard_number = motherboard_number\n",
    "        self.firmware_version = firmware_version\n",
    "        self.timestamps = timestamps\n",
    "        self.delta_window = delta_window\n",
    "        self.include_offset = include_offset\n",
    "        self.apply_calibration = apply_calibration\n",
    "        self.apply_mask = apply_mask\n",
    "        self.absolute_timestamps = absolute_timestamps\n",
    "        self.number_of_cores = number_of_cores\n",
    "\n",
    "        os.chdir(self.path)\n",
    "\n",
    "        # Load calibration if requested\n",
    "        if self.apply_calibration:\n",
    "\n",
    "            # work_dir = Path(__file__).resolve().parent.parent\n",
    "\n",
    "            # TODO\n",
    "            # path_calibration_data = os.path.join(\n",
    "            #     work_dir, r\"params\\calibration_data\"\n",
    "            # )\n",
    "            path_calibration_data = (\n",
    "                r\"/home/sj/GitHub/daplis/src/daplis/params/calibration_data\"\n",
    "            )\n",
    "            # path_calibration_data = r\"C:\\Users\\bruce\\Documents\\GitHub\\daplis\\src\\daplis\\params\\calibration_data\"\n",
    "\n",
    "            calibration_data = load_calibration_data(\n",
    "                path_calibration_data,\n",
    "                daughterboard_number,\n",
    "                motherboard_number,\n",
    "                firmware_version,\n",
    "                include_offset,\n",
    "            )\n",
    "\n",
    "            if self.include_offset:\n",
    "                self.calibration_matrix, self.offset_array = calibration_data\n",
    "            else:\n",
    "                self.calibration_matrix = calibration_data\n",
    "\n",
    "        # Apply mask if requested\n",
    "        if self.apply_mask:\n",
    "            mask = utils.apply_mask(\n",
    "                self.daughterboard_number,\n",
    "                self.motherboard_number,\n",
    "            )\n",
    "            if isinstance(self.pixels[0], int) and isinstance(\n",
    "                self.pixels[1], int\n",
    "            ):\n",
    "                self.pixels = [pix for pix in self.pixels if pix not in mask]\n",
    "            else:\n",
    "                self.pixels = [\n",
    "                    [value for value in sublist if value not in mask]\n",
    "                    for sublist in pixels\n",
    "                ]\n",
    "\n",
    "        # Check the firmware version and set the pixel coordinates accordingly\n",
    "        if self.firmware_version == \"2212s\":\n",
    "            self.pix_coor = np.arange(256).reshape(4, 64).T\n",
    "        elif firmware_version == \"2212b\":\n",
    "            self.pix_coor = np.arange(256).reshape(64, 4)\n",
    "        else:\n",
    "            print(\"\\nFirmware version is not recognized.\")\n",
    "            sys.exit()\n",
    "\n",
    "    def _unpack_binary_data(\n",
    "        self,\n",
    "        file: str,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Unpack binary data from LinoSPAD2.\n",
    "\n",
    "        Same unpacking function as the standard 'daplis' one, except\n",
    "        the calibration matrix is preloaded during initialization and\n",
    "        called here. Return a 3D matrix of pixel numbers and timestamps.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file : str\n",
    "            A '.dat' data file from LinoSPAD2 with the binary-encoded\n",
    "            data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            A 3D matrix of the pixel numbers where timestamp was\n",
    "            recorded and timestamps themselves.\n",
    "        \"\"\"\n",
    "        # Unpack binary data\n",
    "        raw_data = np.memmap(file, dtype=np.uint32)\n",
    "        # Timestamps are stored in the lower 28 bits\n",
    "        data_timestamps = (raw_data & 0xFFFFFFF).astype(np.int64)\n",
    "        # Pixel address in the given TDC is 2 bits above timestamp\n",
    "        data_pixels = ((raw_data >> 28) & 0x3).astype(np.int8)\n",
    "        # Check the top bit, assign '-1' to invalid timestamps\n",
    "        data_timestamps[raw_data < 0x80000000] = -1\n",
    "\n",
    "        # Number of acquisition cycles in each data file\n",
    "        cycles = len(data_timestamps) // (self.timestamps * 65)\n",
    "        # Transform into a matrix of size 65 by cycles*timestamps\n",
    "        data_pixels = (\n",
    "            data_pixels.reshape(cycles, 65, self.timestamps)\n",
    "            .transpose((1, 0, 2))\n",
    "            .reshape(65, -1)\n",
    "        )\n",
    "\n",
    "        data_timestamps = (\n",
    "            data_timestamps.reshape(cycles, 65, self.timestamps)\n",
    "            .transpose((1, 0, 2))\n",
    "            .reshape(65, -1)\n",
    "        )\n",
    "\n",
    "        # Cut the 65th TDC that does not hold any actual data from pixels\n",
    "        data_pixels = data_pixels[:-1]\n",
    "        data_timestamps = data_timestamps[:-1]\n",
    "\n",
    "        # Insert '-2' at the end of each cycle\n",
    "        insert_indices = np.linspace(\n",
    "            self.timestamps, cycles * self.timestamps, cycles\n",
    "        ).astype(np.int64)\n",
    "\n",
    "        data_pixels = np.insert(\n",
    "            data_pixels,\n",
    "            insert_indices,\n",
    "            -2,\n",
    "            1,\n",
    "        )\n",
    "        data_timestamps = np.insert(\n",
    "            data_timestamps,\n",
    "            insert_indices,\n",
    "            -2,\n",
    "            1,\n",
    "        )\n",
    "\n",
    "        # Combine both matrices into a single one, where each cell holds pixel\n",
    "        # coordinates in the TDC and the timestamp\n",
    "        data_all = np.stack((data_pixels, data_timestamps), axis=2).astype(\n",
    "            np.int64\n",
    "        )\n",
    "\n",
    "        if self.apply_calibration is False:\n",
    "            data_all[:, :, 1] = data_all[:, :, 1] * 2500 / 140\n",
    "        else:\n",
    "            # Path to the calibration data\n",
    "            pix_coordinates = np.arange(256).reshape(64, 4)\n",
    "            for i in range(256):\n",
    "                # Transform pixel number to TDC number and pixel\n",
    "                # coordinates in that TDC (from 0 to 3)\n",
    "                tdc, pix = np.argwhere(pix_coordinates == i)[0]\n",
    "                # Find data from that pixel\n",
    "                ind = np.where(data_all[tdc].T[0] == pix)[0]\n",
    "                # Cut non-valid timestamps ('-1's)\n",
    "                ind = ind[data_all[tdc].T[1][ind] >= 0]\n",
    "                if not np.any(ind):\n",
    "                    continue\n",
    "                data_cut = data_all[tdc].T[1][ind]\n",
    "                # Apply calibration; offset is added due to how delta\n",
    "                # ts are calculated\n",
    "                if self.include_offset:\n",
    "                    data_all[tdc].T[1][ind] = (\n",
    "                        (data_cut - data_cut % 140) * 2500 / 140\n",
    "                        + self.calibration_matrix[i, (data_cut % 140)]\n",
    "                        + self.offset_array[i]\n",
    "                    )\n",
    "                else:\n",
    "                    data_all[tdc].T[1][ind] = (\n",
    "                        data_cut - data_cut % 140\n",
    "                    ) * 2500 / 140 + self.calibration_matrix[\n",
    "                        i, (data_cut % 140)\n",
    "                    ]\n",
    "\n",
    "        return data_all\n",
    "\n",
    "    def _calculate_differences_2212_fast(\n",
    "        self,\n",
    "        data: ndarray,\n",
    "        delta_window: float = 50e3,\n",
    "        cycle_length: float = 4e9,\n",
    "    ):\n",
    "        \"\"\"Calculate timestamp differences for firmware version 2212.\n",
    "\n",
    "        Calculate timestamp differences for the given pixels and LinoSPAD2\n",
    "        firmware version 2212. Modified compared to the standard 'daplis'\n",
    "        one. Modifications are for working with prechunked data, i.e.,\n",
    "        sliced down to two arrays from the whole 64xN matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : ndarray\n",
    "            Matrix of timestamps, where rows correspond to the TDCs.\n",
    "        pixels : List[int] | List[List[int]]\n",
    "            List of pixel numbers for which the timestamp differences should\n",
    "            be calculated or list of two lists with pixel numbers for peak\n",
    "            vs. peak calculations.\n",
    "        pix_coor : ndarray\n",
    "            Array for transforming the pixel address in terms of TDC (0 to 3)\n",
    "            to pixel number in terms of half of the sensor (0 to 255).\n",
    "        delta_window : float, optional\n",
    "            Width of the time window for counting timestamp differences.\n",
    "            The default is 50e3 (50 ns).\n",
    "        cycle_length : float, optional\n",
    "            Length of each acquisition cycle. The default is 4e9 (4 ms).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        deltas_all : dict\n",
    "            Dictionary containing timestamp differences for each pair of pixels.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Dictionary for the timestamp differences, where keys are the\n",
    "        # pixel numbers of the requested pairs\n",
    "\n",
    "        deltas_all = []\n",
    "\n",
    "        data_pix_1 = data[0]\n",
    "        data_pix_2 = data[1]\n",
    "\n",
    "        indices1 = data_pix_1[0]\n",
    "        indices2 = data_pix_2[0]\n",
    "        timestamps1 = data_pix_1[1]\n",
    "        timestamps2 = data_pix_2[1]\n",
    "\n",
    "        timestamps_1 = []\n",
    "        timestamps_2 = []\n",
    "\n",
    "        # Go over cycles, shifting the timestamps from each next\n",
    "        # cycle by lengths of cycles before (e.g., for the 4th cycle\n",
    "        # add 12 ms)\n",
    "        for i, _ in enumerate(self.cycle_ends[:-1]):\n",
    "            slice_from = self.cycle_ends[i]\n",
    "            slice_to = self.cycle_ends[i + 1]\n",
    "            pix1_slice = indices1[\n",
    "                (indices1 >= slice_from) & (indices1 < slice_to)\n",
    "            ]\n",
    "            if not np.any(pix1_slice):\n",
    "                continue\n",
    "            pix2_slice = indices2[\n",
    "                (indices2 >= slice_from) & (indices2 < slice_to)\n",
    "            ]\n",
    "            if not np.any(pix2_slice):\n",
    "                continue\n",
    "\n",
    "            # Shift timestamps by cycle length\n",
    "            tmsp1 = timestamps1[np.isin(indices1, pix1_slice)]\n",
    "            tmsp1 = tmsp1[tmsp1 > 0]\n",
    "            tmsp1 = tmsp1 + cycle_length * i\n",
    "\n",
    "            tmsp2 = timestamps2[np.isin(indices2, pix2_slice)]\n",
    "            tmsp2 = tmsp2[tmsp2 > 0]\n",
    "            tmsp2 = tmsp2 + cycle_length * i\n",
    "\n",
    "            timestamps_1.extend(tmsp1)\n",
    "            timestamps_2.extend(tmsp2)\n",
    "\n",
    "        timestamps_1 = np.array(timestamps_1)\n",
    "        timestamps_2 = np.array(timestamps_2)\n",
    "\n",
    "        # Indicators for each pixel: 0 for timestamps from one pixel\n",
    "        # 1 - from the other\n",
    "        pix1_ind = np.zeros(len(timestamps_1), dtype=np.int32)\n",
    "        pix2_ind = np.ones(len(timestamps_2), dtype=np.int32)\n",
    "\n",
    "        pix1_data = np.vstack((pix1_ind, timestamps_1))\n",
    "        pix2_data = np.vstack((pix2_ind, timestamps_2))\n",
    "\n",
    "        # Dataframe for each pixel with pixel indicator and\n",
    "        # timestamps\n",
    "        df1 = pd.DataFrame(pix1_data.T, columns=[\"Pixel_index\", \"Timestamp\"])\n",
    "        df2 = pd.DataFrame(pix2_data.T, columns=[\"Pixel_index\", \"Timestamp\"])\n",
    "\n",
    "        # Combine the two dataframes\n",
    "        df_combined = pd.concat((df1, df2), ignore_index=True)\n",
    "\n",
    "        # Sort the timestamps\n",
    "        df_combined.sort_values(\"Timestamp\", inplace=True)\n",
    "\n",
    "        # Subtract pixel indicators of neighbors; values of 0\n",
    "        # correspond to timestamp differences for the same pixel\n",
    "        # '-1' and '1' - to differences from different pixels\n",
    "        df_combined[\"Pixel_index_diff\"] = df_combined[\"Pixel_index\"].diff()\n",
    "\n",
    "        # Calculate timestamp difference between neighbors\n",
    "        df_combined[\"Timestamp_diff\"] = df_combined[\"Timestamp\"].diff()\n",
    "\n",
    "        # Get the correct timestamp difference sign\n",
    "        df_combined[\"Timestamp_diff\"] = (\n",
    "            df_combined[\"Timestamp_diff\"] * df_combined[\"Pixel_index_diff\"]\n",
    "        )\n",
    "\n",
    "        # Collect timestamp differences where timestamps are from\n",
    "        # different pixels\n",
    "        filtered_df = df_combined[abs(df_combined[\"Pixel_index_diff\"]) == 1]\n",
    "\n",
    "        # Save only timestamps differences in the requested window\n",
    "        delta_ts = filtered_df[\n",
    "            abs(filtered_df[\"Timestamp_diff\"]) < delta_window\n",
    "        ][\"Timestamp_diff\"].values\n",
    "\n",
    "        deltas_all.extend(delta_ts)\n",
    "\n",
    "        return deltas_all\n",
    "\n",
    "    def _calculate_timestamps_differences(self, args):\n",
    "        \"\"\"Calculate photon coincidences and save to '.feather'.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        #TODO\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"From The Process\", time.time())\n",
    "\n",
    "        # try-except railguard for a function that goes to separate\n",
    "        # cores\n",
    "        file, data, pixel_pair = args\n",
    "        try:\n",
    "            # Check if the 'delta_ts_data' folder exists\n",
    "            output_dir = Path(self.path) / \"delta_ts_data\"\n",
    "            output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "            # Calculate the differences and convert them to a pandas\n",
    "            # dataframe - standard approach sending the whole matrix of\n",
    "            # 64xN to each child process\n",
    "            deltas_all = cd.calculate_differences_2212_fast(\n",
    "                data, pixel_pair, self.pix_coor\n",
    "            )\n",
    "\n",
    "            # Calculate the differences and convert them to a pandas\n",
    "            # dataframe - pre-chunking the data down to 2 arrays\n",
    "            # deltas_all = self._calculate_differences_2212_fast(data)\n",
    "            \n",
    "            data_for_plot_df = pd.DataFrame(\n",
    "                deltas_all, columns=[f\"{pixel_pair[0]},{pixel_pair[1]}\"]\n",
    "            ).T\n",
    "\n",
    "            # Save the data to a '.feather' file\n",
    "            file_name = Path(\n",
    "                file\n",
    "            ).stem  # Get the file name without the extension\n",
    "            output_file = (\n",
    "                output_dir\n",
    "                / f\"{file_name}_{pixel_pair[0]}_{pixel_pair[1]}.feather\"\n",
    "            )\n",
    "            file_name = Path(file).stem\n",
    "            output_file = (\n",
    "                output_dir\n",
    "                / f\"{file_name}_{pixel_pair[0]}_{pixel_pair[1]}.feather\"\n",
    "            )\n",
    "            ft.write_feather(\n",
    "                data_for_plot_df.reset_index(drop=True), output_file\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "    def _combine_feather_files(self, path_to_feather_files: str):\n",
    "\n",
    "        os.chdir(path_to_feather_files)\n",
    "\n",
    "        for pixel_pair in self.pixel_pairs:\n",
    "            ft_files = glob.glob(f\"*{pixel_pair[0]}_{pixel_pair[1]}*.feather\")\n",
    "            data_all = pd.DataFrame()\n",
    "            for ft_file in ft_files:\n",
    "                data = ft.read_feather(ft_file)\n",
    "                data_all = pd.concat((data_all, data), ignore_index=True)\n",
    "            data_all.to_feather(\n",
    "                f\"combined_{pixel_pair[0]}_{pixel_pair[1]}.feather\"\n",
    "            )\n",
    "\n",
    "    ### Heavy data chunking - fast calculation in each child process at\n",
    "    ### the cost of long chunking - no acceleration as a result\n",
    "    # def _chunk_that_data_too_much(self, data, pixel_pair, cycle_length: float = 4e9):\n",
    "\n",
    "    #     pix_coor = np.arange(256).reshape(64, 4)\n",
    "\n",
    "    #     # Find ends of cycles\n",
    "    #     cycle_ends = np.argwhere(data[0].T[0] == -2)\n",
    "    #     cycle_ends = np.insert(cycle_ends, 0, 0)\n",
    "\n",
    "    #     tdc1, pix_c1 = np.argwhere(pix_coor == pixel_pair[0])[0]\n",
    "    #     pix1 = np.where(data[tdc1].T[0] == pix_c1)[0]\n",
    "\n",
    "    #     timestamps_1 = []\n",
    "    #     timestamps_2 = []\n",
    "\n",
    "    #     # Second pixel in the pair\n",
    "    #     tdc2, pix_c2 = np.argwhere(pix_coor == pixel_pair[1])[0]\n",
    "    #     pix2 = np.where(data[tdc2].T[0] == pix_c2)[0]\n",
    "\n",
    "    #     # Go over cycles, shifting the timestamps from each next\n",
    "    #     # cycle by lengths of cycles before (e.g., for the 4th cycle\n",
    "    #     # add 12 ms)\n",
    "    #     for i, _ in enumerate(cycle_ends[:-1]):\n",
    "    #         slice_from = cycle_ends[i]\n",
    "    #         slice_to = cycle_ends[i + 1]\n",
    "    #         pix1_slice = pix1[(pix1 >= slice_from) & (pix1 < slice_to)]\n",
    "    #         if not np.any(pix1_slice):\n",
    "    #             continue\n",
    "    #         pix2_slice = pix2[(pix2 >= slice_from) & (pix2 < slice_to)]\n",
    "    #         if not np.any(pix2_slice):\n",
    "    #             continue\n",
    "\n",
    "    #         # Shift timestamps by cycle length\n",
    "    #         tmsp1 = data[tdc1].T[1][pix1_slice]\n",
    "    #         tmsp1 = tmsp1[tmsp1 > 0]\n",
    "    #         tmsp1 = tmsp1 + cycle_length * i\n",
    "\n",
    "    #         tmsp2 = data[tdc2].T[1][pix2_slice]\n",
    "    #         tmsp2 = tmsp2[tmsp2 > 0]\n",
    "    #         tmsp2 = tmsp2 + cycle_length * i\n",
    "\n",
    "    #         timestamps_1.extend(tmsp1)\n",
    "    #         timestamps_2.extend(tmsp2)\n",
    "\n",
    "    #     timestamps_1 = np.array(timestamps_1)\n",
    "    #     timestamps_2 = np.array(timestamps_2)\n",
    "\n",
    "    #     return (timestamps_1, timestamps_2)\n",
    "\n",
    "    def _chunk_that_data(self, data, pixel_pair, cycle_length: float = 4e9):\n",
    "        \"\"\"Chunk data down to 2 rows - prepare for child processes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : np.ndarray\n",
    "            The whole matrix.\n",
    "        pixel_pair : list\n",
    "            Pair of pixels to slice out of the whole matrix.\n",
    "        cycle_length : float, optional\n",
    "            Cycle length in ps, by default 4e9\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            Tuple of two arrays, each contains timestamps from the two\n",
    "            pixels requested and the indices of the timestamps in the\n",
    "            original matrix of data. The indices are used for correct\n",
    "            assigning to the corresponding acquisition cycles.\n",
    "        \"\"\"\n",
    "\n",
    "        tdc1, pix_c1 = np.argwhere(self.pix_coor == pixel_pair[0])[0]\n",
    "        indices1 = np.where(data[tdc1].T[0] == pix_c1)[0]\n",
    "\n",
    "        # Second pixel in the pair\n",
    "        tdc2, pix_c2 = np.argwhere(self.pix_coor == pixel_pair[1])[0]\n",
    "        indices2 = np.where(data[tdc2].T[0] == pix_c2)[0]\n",
    "\n",
    "        data_cut_1 = np.array((indices1, data[tdc1].T[1][indices1]))\n",
    "        data_cut_2 = np.array((indices2, data[tdc2].T[1][indices2]))\n",
    "\n",
    "        return (data_cut_1, data_cut_2)\n",
    "\n",
    "    def calculate_and_save_timestamp_differences_mp(self):\n",
    "\n",
    "        # Find all LinoSPAD2 data files\n",
    "        files = glob.glob(\"*.dat\")\n",
    "\n",
    "        if not files:\n",
    "            raise ValueError(\"No .dat files found in the specified path.\")\n",
    "\n",
    "        self.pixel_pairs = []\n",
    "        for i in self.pixels[0]:\n",
    "            for j in self.pixels[1]:\n",
    "                self.pixel_pairs.append([i, j])\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Go file by file\n",
    "        for file in files:\n",
    "            print(f\"Processing file: {file}\")\n",
    "\n",
    "            start_time_unpack = time.time()\n",
    "\n",
    "            # Unpack the data from the file\n",
    "            data = self._unpack_binary_data(file)\n",
    "\n",
    "            # Pre-collect the indices of the acquisition cycles' ends\n",
    "            self.cycle_ends = np.argwhere(data[0].T[0] == -2)\n",
    "            self.cycle_ends = np.insert(self.cycle_ends, 0, 0)\n",
    "\n",
    "            start_time_args = time.time()\n",
    "\n",
    "            # Prepare the arguments for the child processes:\n",
    "            # 'file' and 'pixel_pair' for naming purposes,\n",
    "            # chunked data as a tuple of two arrays to work with\n",
    "            args = [\n",
    "                (\n",
    "                    file,\n",
    "                    data,\n",
    "                    pixel_pair,\n",
    "                )\n",
    "                for pixel_pair in self.pixel_pairs\n",
    "            ]\n",
    "\n",
    "            # print(f\"Created args in {time.time() - start_time_args}\")\n",
    "            print(\"Before the processes\", time.time())\n",
    "\n",
    "            with multiprocessing.Pool(\n",
    "                min(self.number_of_cores, len(self.pixel_pairs))\n",
    "            ) as pool:\n",
    "                pool.map(\n",
    "                    self._calculate_timestamps_differences,\n",
    "                    args,\n",
    "                )\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(\n",
    "            f\"Parallel processing of files \"\n",
    "            \"files (with each writing to its file) finished \"\n",
    "            f\"in: {round(end_time - start_time, 2)} s\"\n",
    "        )\n",
    "\n",
    "        # Combine '.feather' files from separate cores\n",
    "        path_to_feathers = os.path.join(self.path, \"delta_ts_data\")\n",
    "\n",
    "        self._combine_feather_files(path_to_feathers)\n",
    "\n",
    "        path_output = os.path.join(self.path, \"delta_ts_data\")\n",
    "\n",
    "        print(\n",
    "            \"The feather files with the timestamp differences were \"\n",
    "            f\"combined into the 'combined.feather' file in {path_output}\"\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # path = r\"D:\\LinoSPAD2\\Data\\B7d\\2024.11.11\\500t\\MP_test\"\n",
    "    path = r'/media/sj/King4TB/LS2_Data/2024.11.11/500t/MP_test_70'\n",
    "\n",
    "    mp = MpWizard(\n",
    "        path,\n",
    "        # pixels=[[144], [171, 172]],\n",
    "        pixels=[[x for x in range(55, 60)], [x for x in range(175, 180)]],\n",
    "        # pixels=[[x for x in range(20, 80)], [x for x in range(130, 190)]],\n",
    "        daughterboard_number=\"B7d\",\n",
    "        motherboard_number=\"#28\",\n",
    "        firmware_version=\"2212b\",\n",
    "        timestamps=500,\n",
    "        number_of_cores=5,\n",
    "    )\n",
    "\n",
    "    mp.calculate_and_save_timestamp_differences_mp()\n",
    "\n",
    "\n",
    "### Standard approach - for control and comparison\n",
    "\n",
    "# import time\n",
    "\n",
    "# from daplis.functions import delta_t\n",
    "\n",
    "# time_start = time.time()\n",
    "\n",
    "# path = r\"/media/sj/King4TB/LS2_Data/2024.11.11/500t/MP_test_70\"\n",
    "\n",
    "\n",
    "# delta_t.calculate_and_save_timestamp_differences_fast(\n",
    "#     path,\n",
    "#     rewrite=True,\n",
    "#     # pixels=[144, 171],\n",
    "#     pixels=[[x for x in range(55, 60)], [x for x in range(175, 180)]],\n",
    "#     # pixels=[[x for x in range(20, 80)], [x for x in range(130, 190)]],\n",
    "#     daughterboard_number=\"B7d\",\n",
    "#     motherboard_number=\"#28\",\n",
    "#     firmware_version=\"2212b\",\n",
    "#     timestamps=500,\n",
    "# )\n",
    "\n",
    "# print(f\"Finished in {time.time() - time_start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second approach: preprocess the data before sending to each child process\n",
    "\n",
    "Faster but still not good. Each child process works very fast but preparing the data takes a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: 0000054222.dat\n",
      "Before the processes 1737459558.2638874\n",
      "From The Process 1737459558.303448\n",
      "From The Process 1737459558.3140216\n",
      "From The Process 1737459558.3260505\n",
      "From The Process 1737459558.341788\n",
      "From The Process 1737459558.3604708\n",
      "From The Process 1737459558.628499\n",
      "From The Process 1737459558.6332514\n",
      "From The Process From The Process1737459558.6536448\n",
      " From The Process1737459558.654796 \n",
      "1737459558.6567192\n",
      "From The Process 1737459558.753786\n",
      "From The Process 1737459558.7629943\n",
      "From The Process 1737459558.7910752\n",
      "From The Process 1737459558.8094456\n",
      "From The Process 1737459558.8205626\n",
      "From The Process 1737459559.032863From The Process\n",
      " 1737459559.0349278\n",
      "From The Process 1737459559.06195\n",
      "From The Process From The Process1737459559.0645025\n",
      " 1737459559.065667\n",
      "From The Process 1737459559.2807171\n",
      "From The Process 1737459559.2867618\n",
      "From The Process 1737459559.3289568\n",
      "From The Process From The Process1737459559.4085212 \n",
      "1737459559.409383\n",
      "Parallel processing of files files (with each writing to its file) finished in: 7.77 s\n",
      "The feather files with the timestamp differences were combined into the 'combined.feather' file in /media/sj/King4TB/LS2_Data/2024.11.11/500t/MP_test_70/delta_ts_data\n"
     ]
    }
   ],
   "source": [
    "\"\"\" #TODO\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import glob\n",
    "import multiprocessing\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from daplis.functions import calc_diff as cd\n",
    "from daplis.functions import utils\n",
    "from daplis.functions.calibrate import load_calibration_data\n",
    "from numpy import ndarray\n",
    "from pyarrow import feather as ft\n",
    "\n",
    "\n",
    "class MpWizard:\n",
    "\n",
    "    # Initialize by passing the input parameters which later will be\n",
    "    # passed into all internal functions\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str = \"\",\n",
    "        pixels: list = [],\n",
    "        daughterboard_number: str = \"\",\n",
    "        motherboard_number: str = \"\",\n",
    "        firmware_version: str = \"\",\n",
    "        timestamps: int = 512,\n",
    "        delta_window: float = 50e3,\n",
    "        include_offset: bool = False,\n",
    "        apply_calibration: bool = True,\n",
    "        apply_mask: bool = True,\n",
    "        absolute_timestamps: bool = False,\n",
    "        number_of_cores: int = 1,\n",
    "    ):\n",
    "        \"\"\"Initialization of the class.\n",
    "\n",
    "        Set all the input parameters for later use with internal class\n",
    "        functions. Additionally, preload the calibration matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str, optional\n",
    "            _description_, by default \"\"\n",
    "        pixels : list, optional\n",
    "            _description_, by default []\n",
    "        daughterboard_number : str, optional\n",
    "            _description_, by default \"\"\n",
    "        motherboard_number : str, optional\n",
    "            _description_, by default \"\"\n",
    "        firmware_version : str, optional\n",
    "            _description_, by default \"\"\n",
    "        timestamps : int, optional\n",
    "            _description_, by default 512\n",
    "        delta_window : float, optional\n",
    "            _description_, by default 50e3\n",
    "        include_offset : bool, optional\n",
    "            _description_, by default False\n",
    "        apply_calibration : bool, optional\n",
    "            _description_, by default True\n",
    "        apply_mask : bool, optional\n",
    "            _description_, by default True\n",
    "        absolute_timestamps : bool, optional\n",
    "            _description_, by default False\n",
    "        number_of_cores : int, optional\n",
    "            _description_, by default 1\n",
    "        \"\"\"\n",
    "\n",
    "        self.path = path\n",
    "        self.pixels = pixels\n",
    "        self.daughterboard_number = daughterboard_number\n",
    "        self.motherboard_number = motherboard_number\n",
    "        self.firmware_version = firmware_version\n",
    "        self.timestamps = timestamps\n",
    "        self.delta_window = delta_window\n",
    "        self.include_offset = include_offset\n",
    "        self.apply_calibration = apply_calibration\n",
    "        self.apply_mask = apply_mask\n",
    "        self.absolute_timestamps = absolute_timestamps\n",
    "        self.number_of_cores = number_of_cores\n",
    "\n",
    "        os.chdir(self.path)\n",
    "\n",
    "        # Load calibration if requested\n",
    "        if self.apply_calibration:\n",
    "\n",
    "            # work_dir = Path(__file__).resolve().parent.parent\n",
    "\n",
    "            # TODO\n",
    "            # path_calibration_data = os.path.join(\n",
    "            #     work_dir, r\"params\\calibration_data\"\n",
    "            # )\n",
    "            path_calibration_data = (\n",
    "                r\"/home/sj/GitHub/daplis/src/daplis/params/calibration_data\"\n",
    "            )\n",
    "            # path_calibration_data = r\"C:\\Users\\bruce\\Documents\\GitHub\\daplis\\src\\daplis\\params\\calibration_data\"\n",
    "\n",
    "            calibration_data = load_calibration_data(\n",
    "                path_calibration_data,\n",
    "                daughterboard_number,\n",
    "                motherboard_number,\n",
    "                firmware_version,\n",
    "                include_offset,\n",
    "            )\n",
    "\n",
    "            if self.include_offset:\n",
    "                self.calibration_matrix, self.offset_array = calibration_data\n",
    "            else:\n",
    "                self.calibration_matrix = calibration_data\n",
    "\n",
    "        # Apply mask if requested\n",
    "        if self.apply_mask:\n",
    "            mask = utils.apply_mask(\n",
    "                self.daughterboard_number,\n",
    "                self.motherboard_number,\n",
    "            )\n",
    "            if isinstance(self.pixels[0], int) and isinstance(\n",
    "                self.pixels[1], int\n",
    "            ):\n",
    "                self.pixels = [pix for pix in self.pixels if pix not in mask]\n",
    "            else:\n",
    "                self.pixels = [\n",
    "                    [value for value in sublist if value not in mask]\n",
    "                    for sublist in pixels\n",
    "                ]\n",
    "\n",
    "        # Check the firmware version and set the pixel coordinates accordingly\n",
    "        if self.firmware_version == \"2212s\":\n",
    "            self.pix_coor = np.arange(256).reshape(4, 64).T\n",
    "        elif firmware_version == \"2212b\":\n",
    "            self.pix_coor = np.arange(256).reshape(64, 4)\n",
    "        else:\n",
    "            print(\"\\nFirmware version is not recognized.\")\n",
    "            sys.exit()\n",
    "\n",
    "    def _unpack_binary_data(\n",
    "        self,\n",
    "        file: str,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Unpack binary data from LinoSPAD2.\n",
    "\n",
    "        Same unpacking function as the standard 'daplis' one, except\n",
    "        the calibration matrix is preloaded during initialization and\n",
    "        called here. Return a 3D matrix of pixel numbers and timestamps.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file : str\n",
    "            A '.dat' data file from LinoSPAD2 with the binary-encoded\n",
    "            data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            A 3D matrix of the pixel numbers where timestamp was\n",
    "            recorded and timestamps themselves.\n",
    "        \"\"\"\n",
    "        # Unpack binary data\n",
    "        raw_data = np.memmap(file, dtype=np.uint32)\n",
    "        # Timestamps are stored in the lower 28 bits\n",
    "        data_timestamps = (raw_data & 0xFFFFFFF).astype(np.int64)\n",
    "        # Pixel address in the given TDC is 2 bits above timestamp\n",
    "        data_pixels = ((raw_data >> 28) & 0x3).astype(np.int8)\n",
    "        # Check the top bit, assign '-1' to invalid timestamps\n",
    "        data_timestamps[raw_data < 0x80000000] = -1\n",
    "\n",
    "        # Number of acquisition cycles in each data file\n",
    "        cycles = len(data_timestamps) // (self.timestamps * 65)\n",
    "        # Transform into a matrix of size 65 by cycles*timestamps\n",
    "        data_pixels = (\n",
    "            data_pixels.reshape(cycles, 65, self.timestamps)\n",
    "            .transpose((1, 0, 2))\n",
    "            .reshape(65, -1)\n",
    "        )\n",
    "\n",
    "        data_timestamps = (\n",
    "            data_timestamps.reshape(cycles, 65, self.timestamps)\n",
    "            .transpose((1, 0, 2))\n",
    "            .reshape(65, -1)\n",
    "        )\n",
    "\n",
    "        # Cut the 65th TDC that does not hold any actual data from pixels\n",
    "        data_pixels = data_pixels[:-1]\n",
    "        data_timestamps = data_timestamps[:-1]\n",
    "\n",
    "        # Insert '-2' at the end of each cycle\n",
    "        insert_indices = np.linspace(\n",
    "            self.timestamps, cycles * self.timestamps, cycles\n",
    "        ).astype(np.int64)\n",
    "\n",
    "        data_pixels = np.insert(\n",
    "            data_pixels,\n",
    "            insert_indices,\n",
    "            -2,\n",
    "            1,\n",
    "        )\n",
    "        data_timestamps = np.insert(\n",
    "            data_timestamps,\n",
    "            insert_indices,\n",
    "            -2,\n",
    "            1,\n",
    "        )\n",
    "\n",
    "        # Combine both matrices into a single one, where each cell holds pixel\n",
    "        # coordinates in the TDC and the timestamp\n",
    "        data_all = np.stack((data_pixels, data_timestamps), axis=2).astype(\n",
    "            np.int64\n",
    "        )\n",
    "\n",
    "        if self.apply_calibration is False:\n",
    "            data_all[:, :, 1] = data_all[:, :, 1] * 2500 / 140\n",
    "        else:\n",
    "            # Path to the calibration data\n",
    "            pix_coordinates = np.arange(256).reshape(64, 4)\n",
    "            for i in range(256):\n",
    "                # Transform pixel number to TDC number and pixel\n",
    "                # coordinates in that TDC (from 0 to 3)\n",
    "                tdc, pix = np.argwhere(pix_coordinates == i)[0]\n",
    "                # Find data from that pixel\n",
    "                ind = np.where(data_all[tdc].T[0] == pix)[0]\n",
    "                # Cut non-valid timestamps ('-1's)\n",
    "                ind = ind[data_all[tdc].T[1][ind] >= 0]\n",
    "                if not np.any(ind):\n",
    "                    continue\n",
    "                data_cut = data_all[tdc].T[1][ind]\n",
    "                # Apply calibration; offset is added due to how delta\n",
    "                # ts are calculated\n",
    "                if self.include_offset:\n",
    "                    data_all[tdc].T[1][ind] = (\n",
    "                        (data_cut - data_cut % 140) * 2500 / 140\n",
    "                        + self.calibration_matrix[i, (data_cut % 140)]\n",
    "                        + self.offset_array[i]\n",
    "                    )\n",
    "                else:\n",
    "                    data_all[tdc].T[1][ind] = (\n",
    "                        data_cut - data_cut % 140\n",
    "                    ) * 2500 / 140 + self.calibration_matrix[\n",
    "                        i, (data_cut % 140)\n",
    "                    ]\n",
    "\n",
    "        return data_all\n",
    "\n",
    "    def _calculate_differences_2212_fast(\n",
    "        self,\n",
    "        data: ndarray,\n",
    "        delta_window: float = 50e3,\n",
    "        cycle_length: float = 4e9,\n",
    "    ):\n",
    "        \"\"\"Calculate timestamp differences for firmware version 2212.\n",
    "\n",
    "        Calculate timestamp differences for the given pixels and LinoSPAD2\n",
    "        firmware version 2212. Modified compared to the standard 'daplis'\n",
    "        one. Modifications are for working with prechunked data, i.e.,\n",
    "        sliced down to two arrays from the whole 64xN matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : ndarray\n",
    "            Matrix of timestamps, where rows correspond to the TDCs.\n",
    "        pixels : List[int] | List[List[int]]\n",
    "            List of pixel numbers for which the timestamp differences should\n",
    "            be calculated or list of two lists with pixel numbers for peak\n",
    "            vs. peak calculations.\n",
    "        pix_coor : ndarray\n",
    "            Array for transforming the pixel address in terms of TDC (0 to 3)\n",
    "            to pixel number in terms of half of the sensor (0 to 255).\n",
    "        delta_window : float, optional\n",
    "            Width of the time window for counting timestamp differences.\n",
    "            The default is 50e3 (50 ns).\n",
    "        cycle_length : float, optional\n",
    "            Length of each acquisition cycle. The default is 4e9 (4 ms).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        deltas_all : dict\n",
    "            Dictionary containing timestamp differences for each pair of pixels.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Dictionary for the timestamp differences, where keys are the\n",
    "        # pixel numbers of the requested pairs\n",
    "\n",
    "        deltas_all = []\n",
    "\n",
    "        # data_pix_1 = data[0]\n",
    "        # data_pix_2 = data[1]\n",
    "\n",
    "        # indices1 = data_pix_1[0]\n",
    "        # indices2 = data_pix_2[0]\n",
    "        # timestamps1 = data_pix_1[1]\n",
    "        # timestamps2 = data_pix_2[1]\n",
    "\n",
    "        # timestamps_1 = []\n",
    "        # timestamps_2 = []\n",
    "\n",
    "        # # Go over cycles, shifting the timestamps from each next\n",
    "        # # cycle by lengths of cycles before (e.g., for the 4th cycle\n",
    "        # # add 12 ms)\n",
    "        # for i, _ in enumerate(self.cycle_ends[:-1]):\n",
    "        #     slice_from = self.cycle_ends[i]\n",
    "        #     slice_to = self.cycle_ends[i + 1]\n",
    "        #     pix1_slice = indices1[\n",
    "        #         (indices1 >= slice_from) & (indices1 < slice_to)\n",
    "        #     ]\n",
    "        #     if not np.any(pix1_slice):\n",
    "        #         continue\n",
    "        #     pix2_slice = indices2[\n",
    "        #         (indices2 >= slice_from) & (indices2 < slice_to)\n",
    "        #     ]\n",
    "        #     if not np.any(pix2_slice):\n",
    "        #         continue\n",
    "\n",
    "        #     # Shift timestamps by cycle length\n",
    "        #     tmsp1 = timestamps1[np.isin(indices1, pix1_slice)]\n",
    "        #     tmsp1 = tmsp1[tmsp1 > 0]\n",
    "        #     tmsp1 = tmsp1 + cycle_length * i\n",
    "\n",
    "        #     tmsp2 = timestamps2[np.isin(indices2, pix2_slice)]\n",
    "        #     tmsp2 = tmsp2[tmsp2 > 0]\n",
    "        #     tmsp2 = tmsp2 + cycle_length * i\n",
    "\n",
    "        #     timestamps_1.extend(tmsp1)\n",
    "        #     timestamps_2.extend(tmsp2)\n",
    "\n",
    "        timestamps_1 = data[0]\n",
    "        timestamps_2 = data[1]\n",
    "\n",
    "        # Indicators for each pixel: 0 for timestamps from one pixel\n",
    "        # 1 - from the other\n",
    "        pix1_ind = np.zeros(len(timestamps_1), dtype=np.int32)\n",
    "        pix2_ind = np.ones(len(timestamps_2), dtype=np.int32)\n",
    "\n",
    "        pix1_data = np.vstack((pix1_ind, timestamps_1))\n",
    "        pix2_data = np.vstack((pix2_ind, timestamps_2))\n",
    "\n",
    "        # Dataframe for each pixel with pixel indicator and\n",
    "        # timestamps\n",
    "        df1 = pd.DataFrame(pix1_data.T, columns=[\"Pixel_index\", \"Timestamp\"])\n",
    "        df2 = pd.DataFrame(pix2_data.T, columns=[\"Pixel_index\", \"Timestamp\"])\n",
    "\n",
    "        # Combine the two dataframes\n",
    "        df_combined = pd.concat((df1, df2), ignore_index=True)\n",
    "\n",
    "        # Sort the timestamps\n",
    "        df_combined.sort_values(\"Timestamp\", inplace=True)\n",
    "\n",
    "        # Subtract pixel indicators of neighbors; values of 0\n",
    "        # correspond to timestamp differences for the same pixel\n",
    "        # '-1' and '1' - to differences from different pixels\n",
    "        df_combined[\"Pixel_index_diff\"] = df_combined[\"Pixel_index\"].diff()\n",
    "\n",
    "        # Calculate timestamp difference between neighbors\n",
    "        df_combined[\"Timestamp_diff\"] = df_combined[\"Timestamp\"].diff()\n",
    "\n",
    "        # Get the correct timestamp difference sign\n",
    "        df_combined[\"Timestamp_diff\"] = (\n",
    "            df_combined[\"Timestamp_diff\"] * df_combined[\"Pixel_index_diff\"]\n",
    "        )\n",
    "\n",
    "        # Collect timestamp differences where timestamps are from\n",
    "        # different pixels\n",
    "        filtered_df = df_combined[abs(df_combined[\"Pixel_index_diff\"]) == 1]\n",
    "\n",
    "        # Save only timestamps differences in the requested window\n",
    "        delta_ts = filtered_df[\n",
    "            abs(filtered_df[\"Timestamp_diff\"]) < delta_window\n",
    "        ][\"Timestamp_diff\"].values\n",
    "\n",
    "        deltas_all.extend(delta_ts)\n",
    "\n",
    "        return deltas_all\n",
    "\n",
    "    def _calculate_timestamps_differences(self, args):\n",
    "        \"\"\"Calculate photon coincidences and save to '.feather'.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        #TODO\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"From The Process\", time.time())\n",
    "\n",
    "        # try-except railguard for a function that goes to separate\n",
    "        # cores\n",
    "        file, data, pixel_pair = args\n",
    "        try:\n",
    "            # Check if the 'delta_ts_data' folder exists\n",
    "            output_dir = Path(self.path) / \"delta_ts_data\"\n",
    "            output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "            # Calculate the differences and convert them to a pandas\n",
    "            # dataframe - standard approach sending the whole matrix of\n",
    "            # 64xN to each child process\n",
    "            # deltas_all = cd.calculate_differences_2212_fast(\n",
    "            #     data, pixel_pair, self.pix_coor\n",
    "            # )\n",
    "\n",
    "            # Calculate the differences and convert them to a pandas\n",
    "            # dataframe - pre-chunking the data down to 2 arrays\n",
    "            deltas_all = self._calculate_differences_2212_fast(data)\n",
    "            \n",
    "            data_for_plot_df = pd.DataFrame(\n",
    "                deltas_all, columns=[f\"{pixel_pair[0]},{pixel_pair[1]}\"]\n",
    "            ).T\n",
    "\n",
    "            # Save the data to a '.feather' file\n",
    "            file_name = Path(\n",
    "                file\n",
    "            ).stem  # Get the file name without the extension\n",
    "            output_file = (\n",
    "                output_dir\n",
    "                / f\"{file_name}_{pixel_pair[0]}_{pixel_pair[1]}.feather\"\n",
    "            )\n",
    "            file_name = Path(file).stem\n",
    "            output_file = (\n",
    "                output_dir\n",
    "                / f\"{file_name}_{pixel_pair[0]}_{pixel_pair[1]}.feather\"\n",
    "            )\n",
    "            ft.write_feather(\n",
    "                data_for_plot_df.reset_index(drop=True), output_file\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "    def _combine_feather_files(self, path_to_feather_files: str):\n",
    "\n",
    "        os.chdir(path_to_feather_files)\n",
    "\n",
    "        for pixel_pair in self.pixel_pairs:\n",
    "            ft_files = glob.glob(f\"*{pixel_pair[0]}_{pixel_pair[1]}*.feather\")\n",
    "            data_all = pd.DataFrame()\n",
    "            for ft_file in ft_files:\n",
    "                data = ft.read_feather(ft_file)\n",
    "                data_all = pd.concat((data_all, data), ignore_index=True)\n",
    "            data_all.to_feather(\n",
    "                f\"combined_{pixel_pair[0]}_{pixel_pair[1]}.feather\"\n",
    "            )\n",
    "\n",
    "    ## Heavy data chunking - fast calculation in each child process at\n",
    "    ## the cost of long chunking - no acceleration as a result\n",
    "    def _chunk_that_data_a_lot(self, data, pixel_pair, cycle_length: float = 4e9):\n",
    "\n",
    "        pix_coor = np.arange(256).reshape(64, 4)\n",
    "\n",
    "        # Find ends of cycles\n",
    "        cycle_ends = np.argwhere(data[0].T[0] == -2)\n",
    "        cycle_ends = np.insert(cycle_ends, 0, 0)\n",
    "\n",
    "        tdc1, pix_c1 = np.argwhere(pix_coor == pixel_pair[0])[0]\n",
    "        pix1 = np.where(data[tdc1].T[0] == pix_c1)[0]\n",
    "\n",
    "        timestamps_1 = []\n",
    "        timestamps_2 = []\n",
    "\n",
    "        # Second pixel in the pair\n",
    "        tdc2, pix_c2 = np.argwhere(pix_coor == pixel_pair[1])[0]\n",
    "        pix2 = np.where(data[tdc2].T[0] == pix_c2)[0]\n",
    "\n",
    "        # Go over cycles, shifting the timestamps from each next\n",
    "        # cycle by lengths of cycles before (e.g., for the 4th cycle\n",
    "        # add 12 ms)\n",
    "        for i, _ in enumerate(cycle_ends[:-1]):\n",
    "            slice_from = cycle_ends[i]\n",
    "            slice_to = cycle_ends[i + 1]\n",
    "            pix1_slice = pix1[(pix1 >= slice_from) & (pix1 < slice_to)]\n",
    "            if not np.any(pix1_slice):\n",
    "                continue\n",
    "            pix2_slice = pix2[(pix2 >= slice_from) & (pix2 < slice_to)]\n",
    "            if not np.any(pix2_slice):\n",
    "                continue\n",
    "\n",
    "            # Shift timestamps by cycle length\n",
    "            tmsp1 = data[tdc1].T[1][pix1_slice]\n",
    "            tmsp1 = tmsp1[tmsp1 > 0]\n",
    "            tmsp1 = tmsp1 + cycle_length * i\n",
    "\n",
    "            tmsp2 = data[tdc2].T[1][pix2_slice]\n",
    "            tmsp2 = tmsp2[tmsp2 > 0]\n",
    "            tmsp2 = tmsp2 + cycle_length * i\n",
    "\n",
    "            timestamps_1.extend(tmsp1)\n",
    "            timestamps_2.extend(tmsp2)\n",
    "\n",
    "        timestamps_1 = np.array(timestamps_1)\n",
    "        timestamps_2 = np.array(timestamps_2)\n",
    "\n",
    "        return (timestamps_1, timestamps_2)\n",
    "\n",
    "    # def _chunk_that_data(self, data, pixel_pair, cycle_length: float = 4e9):\n",
    "    #     \"\"\"Chunk data down to 2 rows - prepare for child processes.\n",
    "\n",
    "    #     Parameters\n",
    "    #     ----------\n",
    "    #     data : np.ndarray\n",
    "    #         The whole matrix.\n",
    "    #     pixel_pair : list\n",
    "    #         Pair of pixels to slice out of the whole matrix.\n",
    "    #     cycle_length : float, optional\n",
    "    #         Cycle length in ps, by default 4e9\n",
    "\n",
    "    #     Returns\n",
    "    #     -------\n",
    "    #     tuple\n",
    "    #         Tuple of two arrays, each contains timestamps from the two\n",
    "    #         pixels requested and the indices of the timestamps in the\n",
    "    #         original matrix of data. The indices are used for correct\n",
    "    #         assigning to the corresponding acquisition cycles.\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     tdc1, pix_c1 = np.argwhere(self.pix_coor == pixel_pair[0])[0]\n",
    "    #     indices1 = np.where(data[tdc1].T[0] == pix_c1)[0]\n",
    "\n",
    "    #     # Second pixel in the pair\n",
    "    #     tdc2, pix_c2 = np.argwhere(self.pix_coor == pixel_pair[1])[0]\n",
    "    #     indices2 = np.where(data[tdc2].T[0] == pix_c2)[0]\n",
    "\n",
    "    #     data_cut_1 = np.array((indices1, data[tdc1].T[1][indices1]))\n",
    "    #     data_cut_2 = np.array((indices2, data[tdc2].T[1][indices2]))\n",
    "\n",
    "    #     return (data_cut_1, data_cut_2)\n",
    "\n",
    "    def calculate_and_save_timestamp_differences_mp(self):\n",
    "\n",
    "        # Find all LinoSPAD2 data files\n",
    "        files = glob.glob(\"*.dat\")\n",
    "\n",
    "        if not files:\n",
    "            raise ValueError(\"No .dat files found in the specified path.\")\n",
    "\n",
    "        self.pixel_pairs = []\n",
    "        for i in self.pixels[0]:\n",
    "            for j in self.pixels[1]:\n",
    "                self.pixel_pairs.append([i, j])\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Go file by file\n",
    "        for file in files:\n",
    "            print(f\"Processing file: {file}\")\n",
    "\n",
    "            start_time_unpack = time.time()\n",
    "\n",
    "            # Unpack the data from the file\n",
    "            data = self._unpack_binary_data(file)\n",
    "\n",
    "            # Pre-collect the indices of the acquisition cycles' ends\n",
    "            self.cycle_ends = np.argwhere(data[0].T[0] == -2)\n",
    "            self.cycle_ends = np.insert(self.cycle_ends, 0, 0)\n",
    "\n",
    "            start_time_args = time.time()\n",
    "\n",
    "            # Prepare the arguments for the child processes:\n",
    "            # 'file' and 'pixel_pair' for naming purposes,\n",
    "            # chunked data as a tuple of two arrays to work with\n",
    "            args = [\n",
    "                (\n",
    "                    file,\n",
    "                    self._chunk_that_data_a_lot(data, pixel_pair),\n",
    "                    pixel_pair,\n",
    "                )\n",
    "                for pixel_pair in self.pixel_pairs\n",
    "            ]\n",
    "\n",
    "            # print(f\"Created args in {time.time() - start_time_args}\")\n",
    "            print(\"Before the processes\", time.time())\n",
    "\n",
    "            with multiprocessing.Pool(\n",
    "                min(self.number_of_cores, len(self.pixel_pairs))\n",
    "            ) as pool:\n",
    "                pool.map(\n",
    "                    self._calculate_timestamps_differences,\n",
    "                    args,\n",
    "                )\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(\n",
    "            f\"Parallel processing of files \"\n",
    "            \"files (with each writing to its file) finished \"\n",
    "            f\"in: {round(end_time - start_time, 2)} s\"\n",
    "        )\n",
    "\n",
    "        # Combine '.feather' files from separate cores\n",
    "        path_to_feathers = os.path.join(self.path, \"delta_ts_data\")\n",
    "\n",
    "        self._combine_feather_files(path_to_feathers)\n",
    "\n",
    "        path_output = os.path.join(self.path, \"delta_ts_data\")\n",
    "\n",
    "        print(\n",
    "            \"The feather files with the timestamp differences were \"\n",
    "            f\"combined into the 'combined.feather' file in {path_output}\"\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # path = r\"D:\\LinoSPAD2\\Data\\B7d\\2024.11.11\\500t\\MP_test\"\n",
    "    path = r'/media/sj/King4TB/LS2_Data/2024.11.11/500t/MP_test_70'\n",
    "\n",
    "    mp = MpWizard(\n",
    "        path,\n",
    "        # pixels=[[144], [171, 172]],\n",
    "        pixels=[[x for x in range(55, 60)], [x for x in range(175, 180)]],\n",
    "        # pixels=[[x for x in range(20, 80)], [x for x in range(130, 190)]],\n",
    "        daughterboard_number=\"B7d\",\n",
    "        motherboard_number=\"#28\",\n",
    "        firmware_version=\"2212b\",\n",
    "        timestamps=500,\n",
    "        number_of_cores=5,\n",
    "    )\n",
    "\n",
    "    mp.calculate_and_save_timestamp_differences_mp()\n",
    "\n",
    "\n",
    "### Standard approach - for control and comparison\n",
    "\n",
    "# import time\n",
    "\n",
    "# from daplis.functions import delta_t\n",
    "\n",
    "# time_start = time.time()\n",
    "\n",
    "# path = r\"/media/sj/King4TB/LS2_Data/2024.11.11/500t/MP_test_70\"\n",
    "\n",
    "\n",
    "# delta_t.calculate_and_save_timestamp_differences_fast(\n",
    "#     path,\n",
    "#     rewrite=True,\n",
    "#     # pixels=[144, 171],\n",
    "#     pixels=[[x for x in range(55, 60)], [x for x in range(175, 180)]],\n",
    "#     # pixels=[[x for x in range(20, 80)], [x for x in range(130, 190)]],\n",
    "#     daughterboard_number=\"B7d\",\n",
    "#     motherboard_number=\"#28\",\n",
    "#     firmware_version=\"2212b\",\n",
    "#     timestamps=500,\n",
    "# )\n",
    "\n",
    "# print(f\"Finished in {time.time() - time_start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third approach: prepare the data but not that much\n",
    "\n",
    "Select the timestamps from the two requested pixels while keeping the indices of the timestamps, i.e., their position. Important for aligning the acquisition cycles.\n",
    "\n",
    "Fastest yet but still on par with the standard sequential approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: 0000054222.dat\n",
      "Before the processes 1737459844.5941215\n",
      "From The Process 1737459844.6574094\n",
      "From The Process 1737459844.6683671\n",
      "From The Process 1737459844.6839461\n",
      "From The Process 1737459844.715067\n",
      "From The Process 1737459844.7342224\n",
      "From The Process 1737459845.1763504\n",
      "From The Process 1737459845.1893883\n",
      "From The Process 1737459845.2241685\n",
      "From The Process 1737459845.374938\n",
      "From The Process 1737459845.7645507\n",
      "From The Process 1737459845.8894908\n",
      "From The Process 1737459845.9427645\n",
      "From The Process 1737459846.1090639\n",
      "From The Process 1737459846.2308009\n",
      "From The Process 1737459846.3279898\n",
      "From The Process 1737459846.504545\n",
      "From The Process 1737459846.5848405\n",
      "From The Process 1737459846.6928904\n",
      "From The Process 1737459846.989765\n",
      "From The Process 1737459847.2066672\n",
      "From The Process 1737459847.2768579\n",
      "From The Process 1737459847.2806358\n",
      "From The Process 1737459847.371996\n",
      "From The Process 1737459847.5272338\n",
      "From The Process 1737459847.797635\n",
      "Parallel processing of files files (with each writing to its file) finished in: 5.2 s\n",
      "The feather files with the timestamp differences were combined into the 'combined.feather' file in /media/sj/King4TB/LS2_Data/2024.11.11/500t/MP_test_70/delta_ts_data\n",
      "\n",
      "> > > Collecting data for delta t plot for the requested pixels and saving it to .feather in a cycle < < <\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting data: 100%|| 1/1 [00:06<00:00,  6.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> > > Timestamp differences are saved as0000054222-0000054222.feather in /media/sj/King4TB/LS2_Data/2024.11.11/500t/MP_test_70/delta_ts_data < < <\n",
      "Finished in 6.534499883651733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" #TODO\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import glob\n",
    "import multiprocessing\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from daplis.functions import calc_diff as cd\n",
    "from daplis.functions import utils\n",
    "from daplis.functions.calibrate import load_calibration_data\n",
    "from numpy import ndarray\n",
    "from pyarrow import feather as ft\n",
    "\n",
    "\n",
    "class MpWizard:\n",
    "\n",
    "    # Initialize by passing the input parameters which later will be\n",
    "    # passed into all internal functions\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str = \"\",\n",
    "        pixels: list = [],\n",
    "        daughterboard_number: str = \"\",\n",
    "        motherboard_number: str = \"\",\n",
    "        firmware_version: str = \"\",\n",
    "        timestamps: int = 512,\n",
    "        delta_window: float = 50e3,\n",
    "        include_offset: bool = False,\n",
    "        apply_calibration: bool = True,\n",
    "        apply_mask: bool = True,\n",
    "        absolute_timestamps: bool = False,\n",
    "        number_of_cores: int = 1,\n",
    "    ):\n",
    "        \"\"\"Initialization of the class.\n",
    "\n",
    "        Set all the input parameters for later use with internal class\n",
    "        functions. Additionally, preload the calibration matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str, optional\n",
    "            _description_, by default \"\"\n",
    "        pixels : list, optional\n",
    "            _description_, by default []\n",
    "        daughterboard_number : str, optional\n",
    "            _description_, by default \"\"\n",
    "        motherboard_number : str, optional\n",
    "            _description_, by default \"\"\n",
    "        firmware_version : str, optional\n",
    "            _description_, by default \"\"\n",
    "        timestamps : int, optional\n",
    "            _description_, by default 512\n",
    "        delta_window : float, optional\n",
    "            _description_, by default 50e3\n",
    "        include_offset : bool, optional\n",
    "            _description_, by default False\n",
    "        apply_calibration : bool, optional\n",
    "            _description_, by default True\n",
    "        apply_mask : bool, optional\n",
    "            _description_, by default True\n",
    "        absolute_timestamps : bool, optional\n",
    "            _description_, by default False\n",
    "        number_of_cores : int, optional\n",
    "            _description_, by default 1\n",
    "        \"\"\"\n",
    "\n",
    "        self.path = path\n",
    "        self.pixels = pixels\n",
    "        self.daughterboard_number = daughterboard_number\n",
    "        self.motherboard_number = motherboard_number\n",
    "        self.firmware_version = firmware_version\n",
    "        self.timestamps = timestamps\n",
    "        self.delta_window = delta_window\n",
    "        self.include_offset = include_offset\n",
    "        self.apply_calibration = apply_calibration\n",
    "        self.apply_mask = apply_mask\n",
    "        self.absolute_timestamps = absolute_timestamps\n",
    "        self.number_of_cores = number_of_cores\n",
    "\n",
    "        os.chdir(self.path)\n",
    "\n",
    "        # Load calibration if requested\n",
    "        if self.apply_calibration:\n",
    "\n",
    "            # work_dir = Path(__file__).resolve().parent.parent\n",
    "\n",
    "            # TODO\n",
    "            # path_calibration_data = os.path.join(\n",
    "            #     work_dir, r\"params\\calibration_data\"\n",
    "            # )\n",
    "            path_calibration_data = (\n",
    "                r\"/home/sj/GitHub/daplis/src/daplis/params/calibration_data\"\n",
    "            )\n",
    "            # path_calibration_data = r\"C:\\Users\\bruce\\Documents\\GitHub\\daplis\\src\\daplis\\params\\calibration_data\"\n",
    "\n",
    "            calibration_data = load_calibration_data(\n",
    "                path_calibration_data,\n",
    "                daughterboard_number,\n",
    "                motherboard_number,\n",
    "                firmware_version,\n",
    "                include_offset,\n",
    "            )\n",
    "\n",
    "            if self.include_offset:\n",
    "                self.calibration_matrix, self.offset_array = calibration_data\n",
    "            else:\n",
    "                self.calibration_matrix = calibration_data\n",
    "\n",
    "        # Apply mask if requested\n",
    "        if self.apply_mask:\n",
    "            mask = utils.apply_mask(\n",
    "                self.daughterboard_number,\n",
    "                self.motherboard_number,\n",
    "            )\n",
    "            if isinstance(self.pixels[0], int) and isinstance(\n",
    "                self.pixels[1], int\n",
    "            ):\n",
    "                self.pixels = [pix for pix in self.pixels if pix not in mask]\n",
    "            else:\n",
    "                self.pixels = [\n",
    "                    [value for value in sublist if value not in mask]\n",
    "                    for sublist in pixels\n",
    "                ]\n",
    "\n",
    "        # Check the firmware version and set the pixel coordinates accordingly\n",
    "        if self.firmware_version == \"2212s\":\n",
    "            self.pix_coor = np.arange(256).reshape(4, 64).T\n",
    "        elif firmware_version == \"2212b\":\n",
    "            self.pix_coor = np.arange(256).reshape(64, 4)\n",
    "        else:\n",
    "            print(\"\\nFirmware version is not recognized.\")\n",
    "            sys.exit()\n",
    "\n",
    "    def _unpack_binary_data(\n",
    "        self,\n",
    "        file: str,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Unpack binary data from LinoSPAD2.\n",
    "\n",
    "        Same unpacking function as the standard 'daplis' one, except\n",
    "        the calibration matrix is preloaded during initialization and\n",
    "        called here. Return a 3D matrix of pixel numbers and timestamps.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file : str\n",
    "            A '.dat' data file from LinoSPAD2 with the binary-encoded\n",
    "            data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            A 3D matrix of the pixel numbers where timestamp was\n",
    "            recorded and timestamps themselves.\n",
    "        \"\"\"\n",
    "        # Unpack binary data\n",
    "        raw_data = np.memmap(file, dtype=np.uint32)\n",
    "        # Timestamps are stored in the lower 28 bits\n",
    "        data_timestamps = (raw_data & 0xFFFFFFF).astype(np.int64)\n",
    "        # Pixel address in the given TDC is 2 bits above timestamp\n",
    "        data_pixels = ((raw_data >> 28) & 0x3).astype(np.int8)\n",
    "        # Check the top bit, assign '-1' to invalid timestamps\n",
    "        data_timestamps[raw_data < 0x80000000] = -1\n",
    "\n",
    "        # Number of acquisition cycles in each data file\n",
    "        cycles = len(data_timestamps) // (self.timestamps * 65)\n",
    "        # Transform into a matrix of size 65 by cycles*timestamps\n",
    "        data_pixels = (\n",
    "            data_pixels.reshape(cycles, 65, self.timestamps)\n",
    "            .transpose((1, 0, 2))\n",
    "            .reshape(65, -1)\n",
    "        )\n",
    "\n",
    "        data_timestamps = (\n",
    "            data_timestamps.reshape(cycles, 65, self.timestamps)\n",
    "            .transpose((1, 0, 2))\n",
    "            .reshape(65, -1)\n",
    "        )\n",
    "\n",
    "        # Cut the 65th TDC that does not hold any actual data from pixels\n",
    "        data_pixels = data_pixels[:-1]\n",
    "        data_timestamps = data_timestamps[:-1]\n",
    "\n",
    "        # Insert '-2' at the end of each cycle\n",
    "        insert_indices = np.linspace(\n",
    "            self.timestamps, cycles * self.timestamps, cycles\n",
    "        ).astype(np.int64)\n",
    "\n",
    "        data_pixels = np.insert(\n",
    "            data_pixels,\n",
    "            insert_indices,\n",
    "            -2,\n",
    "            1,\n",
    "        )\n",
    "        data_timestamps = np.insert(\n",
    "            data_timestamps,\n",
    "            insert_indices,\n",
    "            -2,\n",
    "            1,\n",
    "        )\n",
    "\n",
    "        # Combine both matrices into a single one, where each cell holds pixel\n",
    "        # coordinates in the TDC and the timestamp\n",
    "        data_all = np.stack((data_pixels, data_timestamps), axis=2).astype(\n",
    "            np.int64\n",
    "        )\n",
    "\n",
    "        if self.apply_calibration is False:\n",
    "            data_all[:, :, 1] = data_all[:, :, 1] * 2500 / 140\n",
    "        else:\n",
    "            # Path to the calibration data\n",
    "            pix_coordinates = np.arange(256).reshape(64, 4)\n",
    "            for i in range(256):\n",
    "                # Transform pixel number to TDC number and pixel\n",
    "                # coordinates in that TDC (from 0 to 3)\n",
    "                tdc, pix = np.argwhere(pix_coordinates == i)[0]\n",
    "                # Find data from that pixel\n",
    "                ind = np.where(data_all[tdc].T[0] == pix)[0]\n",
    "                # Cut non-valid timestamps ('-1's)\n",
    "                ind = ind[data_all[tdc].T[1][ind] >= 0]\n",
    "                if not np.any(ind):\n",
    "                    continue\n",
    "                data_cut = data_all[tdc].T[1][ind]\n",
    "                # Apply calibration; offset is added due to how delta\n",
    "                # ts are calculated\n",
    "                if self.include_offset:\n",
    "                    data_all[tdc].T[1][ind] = (\n",
    "                        (data_cut - data_cut % 140) * 2500 / 140\n",
    "                        + self.calibration_matrix[i, (data_cut % 140)]\n",
    "                        + self.offset_array[i]\n",
    "                    )\n",
    "                else:\n",
    "                    data_all[tdc].T[1][ind] = (\n",
    "                        data_cut - data_cut % 140\n",
    "                    ) * 2500 / 140 + self.calibration_matrix[\n",
    "                        i, (data_cut % 140)\n",
    "                    ]\n",
    "\n",
    "        return data_all\n",
    "\n",
    "    def _calculate_differences_2212_fast(\n",
    "        self,\n",
    "        data: ndarray,\n",
    "        delta_window: float = 50e3,\n",
    "        cycle_length: float = 4e9,\n",
    "    ):\n",
    "        \"\"\"Calculate timestamp differences for firmware version 2212.\n",
    "\n",
    "        Calculate timestamp differences for the given pixels and LinoSPAD2\n",
    "        firmware version 2212. Modified compared to the standard 'daplis'\n",
    "        one. Modifications are for working with prechunked data, i.e.,\n",
    "        sliced down to two arrays from the whole 64xN matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : ndarray\n",
    "            Matrix of timestamps, where rows correspond to the TDCs.\n",
    "        pixels : List[int] | List[List[int]]\n",
    "            List of pixel numbers for which the timestamp differences should\n",
    "            be calculated or list of two lists with pixel numbers for peak\n",
    "            vs. peak calculations.\n",
    "        pix_coor : ndarray\n",
    "            Array for transforming the pixel address in terms of TDC (0 to 3)\n",
    "            to pixel number in terms of half of the sensor (0 to 255).\n",
    "        delta_window : float, optional\n",
    "            Width of the time window for counting timestamp differences.\n",
    "            The default is 50e3 (50 ns).\n",
    "        cycle_length : float, optional\n",
    "            Length of each acquisition cycle. The default is 4e9 (4 ms).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        deltas_all : dict\n",
    "            Dictionary containing timestamp differences for each pair of pixels.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Dictionary for the timestamp differences, where keys are the\n",
    "        # pixel numbers of the requested pairs\n",
    "\n",
    "        deltas_all = []\n",
    "\n",
    "        data_pix_1 = data[0]\n",
    "        data_pix_2 = data[1]\n",
    "\n",
    "        indices1 = data_pix_1[0]\n",
    "        indices2 = data_pix_2[0]\n",
    "        timestamps1 = data_pix_1[1]\n",
    "        timestamps2 = data_pix_2[1]\n",
    "\n",
    "        timestamps_1 = []\n",
    "        timestamps_2 = []\n",
    "\n",
    "        # Go over cycles, shifting the timestamps from each next\n",
    "        # cycle by lengths of cycles before (e.g., for the 4th cycle\n",
    "        # add 12 ms)\n",
    "        for i, _ in enumerate(self.cycle_ends[:-1]):\n",
    "            slice_from = self.cycle_ends[i]\n",
    "            slice_to = self.cycle_ends[i + 1]\n",
    "            pix1_slice = indices1[\n",
    "                (indices1 >= slice_from) & (indices1 < slice_to)\n",
    "            ]\n",
    "            if not np.any(pix1_slice):\n",
    "                continue\n",
    "            pix2_slice = indices2[\n",
    "                (indices2 >= slice_from) & (indices2 < slice_to)\n",
    "            ]\n",
    "            if not np.any(pix2_slice):\n",
    "                continue\n",
    "\n",
    "            # Shift timestamps by cycle length\n",
    "            tmsp1 = timestamps1[np.isin(indices1, pix1_slice)]\n",
    "            tmsp1 = tmsp1[tmsp1 > 0]\n",
    "            tmsp1 = tmsp1 + cycle_length * i\n",
    "\n",
    "            tmsp2 = timestamps2[np.isin(indices2, pix2_slice)]\n",
    "            tmsp2 = tmsp2[tmsp2 > 0]\n",
    "            tmsp2 = tmsp2 + cycle_length * i\n",
    "\n",
    "            timestamps_1.extend(tmsp1)\n",
    "            timestamps_2.extend(tmsp2)\n",
    "\n",
    "        timestamps_1 = np.array(timestamps_1)\n",
    "        timestamps_2 = np.array(timestamps_2)\n",
    "\n",
    "        # Indicators for each pixel: 0 for timestamps from one pixel\n",
    "        # 1 - from the other\n",
    "        pix1_ind = np.zeros(len(timestamps_1), dtype=np.int32)\n",
    "        pix2_ind = np.ones(len(timestamps_2), dtype=np.int32)\n",
    "\n",
    "        pix1_data = np.vstack((pix1_ind, timestamps_1))\n",
    "        pix2_data = np.vstack((pix2_ind, timestamps_2))\n",
    "\n",
    "        # Dataframe for each pixel with pixel indicator and\n",
    "        # timestamps\n",
    "        df1 = pd.DataFrame(pix1_data.T, columns=[\"Pixel_index\", \"Timestamp\"])\n",
    "        df2 = pd.DataFrame(pix2_data.T, columns=[\"Pixel_index\", \"Timestamp\"])\n",
    "\n",
    "        # Combine the two dataframes\n",
    "        df_combined = pd.concat((df1, df2), ignore_index=True)\n",
    "\n",
    "        # Sort the timestamps\n",
    "        df_combined.sort_values(\"Timestamp\", inplace=True)\n",
    "\n",
    "        # Subtract pixel indicators of neighbors; values of 0\n",
    "        # correspond to timestamp differences for the same pixel\n",
    "        # '-1' and '1' - to differences from different pixels\n",
    "        df_combined[\"Pixel_index_diff\"] = df_combined[\"Pixel_index\"].diff()\n",
    "\n",
    "        # Calculate timestamp difference between neighbors\n",
    "        df_combined[\"Timestamp_diff\"] = df_combined[\"Timestamp\"].diff()\n",
    "\n",
    "        # Get the correct timestamp difference sign\n",
    "        df_combined[\"Timestamp_diff\"] = (\n",
    "            df_combined[\"Timestamp_diff\"] * df_combined[\"Pixel_index_diff\"]\n",
    "        )\n",
    "\n",
    "        # Collect timestamp differences where timestamps are from\n",
    "        # different pixels\n",
    "        filtered_df = df_combined[abs(df_combined[\"Pixel_index_diff\"]) == 1]\n",
    "\n",
    "        # Save only timestamps differences in the requested window\n",
    "        delta_ts = filtered_df[\n",
    "            abs(filtered_df[\"Timestamp_diff\"]) < delta_window\n",
    "        ][\"Timestamp_diff\"].values\n",
    "\n",
    "        deltas_all.extend(delta_ts)\n",
    "\n",
    "        return deltas_all\n",
    "\n",
    "    def _calculate_timestamps_differences(self, args):\n",
    "        \"\"\"Calculate photon coincidences and save to '.feather'.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        #TODO\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"From The Process\", time.time())\n",
    "\n",
    "        # try-except railguard for a function that goes to separate\n",
    "        # cores\n",
    "        file, data, pixel_pair = args\n",
    "        try:\n",
    "            # Check if the 'delta_ts_data' folder exists\n",
    "            output_dir = Path(self.path) / \"delta_ts_data\"\n",
    "            output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "            # Calculate the differences and convert them to a pandas\n",
    "            # dataframe - standard approach sending the whole matrix of\n",
    "            # 64xN to each child process\n",
    "            # deltas_all = cd.calculate_differences_2212_fast(\n",
    "            #     data, pixel_pair, self.pix_coor\n",
    "            # )\n",
    "\n",
    "            # Calculate the differences and convert them to a pandas\n",
    "            # dataframe - pre-chunking the data down to 2 arrays\n",
    "            deltas_all = self._calculate_differences_2212_fast(data)\n",
    "            \n",
    "            data_for_plot_df = pd.DataFrame(\n",
    "                deltas_all, columns=[f\"{pixel_pair[0]},{pixel_pair[1]}\"]\n",
    "            ).T\n",
    "\n",
    "            # Save the data to a '.feather' file\n",
    "            file_name = Path(\n",
    "                file\n",
    "            ).stem  # Get the file name without the extension\n",
    "            output_file = (\n",
    "                output_dir\n",
    "                / f\"{file_name}_{pixel_pair[0]}_{pixel_pair[1]}.feather\"\n",
    "            )\n",
    "            file_name = Path(file).stem\n",
    "            output_file = (\n",
    "                output_dir\n",
    "                / f\"{file_name}_{pixel_pair[0]}_{pixel_pair[1]}.feather\"\n",
    "            )\n",
    "            ft.write_feather(\n",
    "                data_for_plot_df.reset_index(drop=True), output_file\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "    def _combine_feather_files(self, path_to_feather_files: str):\n",
    "\n",
    "        os.chdir(path_to_feather_files)\n",
    "\n",
    "        for pixel_pair in self.pixel_pairs:\n",
    "            ft_files = glob.glob(f\"*{pixel_pair[0]}_{pixel_pair[1]}*.feather\")\n",
    "            data_all = pd.DataFrame()\n",
    "            for ft_file in ft_files:\n",
    "                data = ft.read_feather(ft_file)\n",
    "                data_all = pd.concat((data_all, data), ignore_index=True)\n",
    "            data_all.to_feather(\n",
    "                f\"combined_{pixel_pair[0]}_{pixel_pair[1]}.feather\"\n",
    "            )\n",
    "\n",
    "    # ## Heavy data chunking - fast calculation in each child process at\n",
    "    # ## the cost of long chunking - no acceleration as a result\n",
    "    # def _chunk_that_data_a_lot(self, data, pixel_pair, cycle_length: float = 4e9):\n",
    "\n",
    "    #     pix_coor = np.arange(256).reshape(64, 4)\n",
    "\n",
    "    #     # Find ends of cycles\n",
    "    #     cycle_ends = np.argwhere(data[0].T[0] == -2)\n",
    "    #     cycle_ends = np.insert(cycle_ends, 0, 0)\n",
    "\n",
    "    #     tdc1, pix_c1 = np.argwhere(pix_coor == pixel_pair[0])[0]\n",
    "    #     pix1 = np.where(data[tdc1].T[0] == pix_c1)[0]\n",
    "\n",
    "    #     timestamps_1 = []\n",
    "    #     timestamps_2 = []\n",
    "\n",
    "    #     # Second pixel in the pair\n",
    "    #     tdc2, pix_c2 = np.argwhere(pix_coor == pixel_pair[1])[0]\n",
    "    #     pix2 = np.where(data[tdc2].T[0] == pix_c2)[0]\n",
    "\n",
    "    #     # Go over cycles, shifting the timestamps from each next\n",
    "    #     # cycle by lengths of cycles before (e.g., for the 4th cycle\n",
    "    #     # add 12 ms)\n",
    "    #     for i, _ in enumerate(cycle_ends[:-1]):\n",
    "    #         slice_from = cycle_ends[i]\n",
    "    #         slice_to = cycle_ends[i + 1]\n",
    "    #         pix1_slice = pix1[(pix1 >= slice_from) & (pix1 < slice_to)]\n",
    "    #         if not np.any(pix1_slice):\n",
    "    #             continue\n",
    "    #         pix2_slice = pix2[(pix2 >= slice_from) & (pix2 < slice_to)]\n",
    "    #         if not np.any(pix2_slice):\n",
    "    #             continue\n",
    "\n",
    "    #         # Shift timestamps by cycle length\n",
    "    #         tmsp1 = data[tdc1].T[1][pix1_slice]\n",
    "    #         tmsp1 = tmsp1[tmsp1 > 0]\n",
    "    #         tmsp1 = tmsp1 + cycle_length * i\n",
    "\n",
    "    #         tmsp2 = data[tdc2].T[1][pix2_slice]\n",
    "    #         tmsp2 = tmsp2[tmsp2 > 0]\n",
    "    #         tmsp2 = tmsp2 + cycle_length * i\n",
    "\n",
    "    #         timestamps_1.extend(tmsp1)\n",
    "    #         timestamps_2.extend(tmsp2)\n",
    "\n",
    "    #     timestamps_1 = np.array(timestamps_1)\n",
    "    #     timestamps_2 = np.array(timestamps_2)\n",
    "\n",
    "    #     return (timestamps_1, timestamps_2)\n",
    "\n",
    "    def _chunk_that_data(self, data, pixel_pair, cycle_length: float = 4e9):\n",
    "        \"\"\"Chunk data down to 2 rows - prepare for child processes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : np.ndarray\n",
    "            The whole matrix.\n",
    "        pixel_pair : list\n",
    "            Pair of pixels to slice out of the whole matrix.\n",
    "        cycle_length : float, optional\n",
    "            Cycle length in ps, by default 4e9\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            Tuple of two arrays, each contains timestamps from the two\n",
    "            pixels requested and the indices of the timestamps in the\n",
    "            original matrix of data. The indices are used for correct\n",
    "            assigning to the corresponding acquisition cycles.\n",
    "        \"\"\"\n",
    "\n",
    "        tdc1, pix_c1 = np.argwhere(self.pix_coor == pixel_pair[0])[0]\n",
    "        indices1 = np.where(data[tdc1].T[0] == pix_c1)[0]\n",
    "\n",
    "        # Second pixel in the pair\n",
    "        tdc2, pix_c2 = np.argwhere(self.pix_coor == pixel_pair[1])[0]\n",
    "        indices2 = np.where(data[tdc2].T[0] == pix_c2)[0]\n",
    "\n",
    "        data_cut_1 = np.array((indices1, data[tdc1].T[1][indices1]))\n",
    "        data_cut_2 = np.array((indices2, data[tdc2].T[1][indices2]))\n",
    "\n",
    "        return (data_cut_1, data_cut_2)\n",
    "\n",
    "    def calculate_and_save_timestamp_differences_mp(self):\n",
    "\n",
    "        # Find all LinoSPAD2 data files\n",
    "        files = glob.glob(\"*.dat\")\n",
    "\n",
    "        if not files:\n",
    "            raise ValueError(\"No .dat files found in the specified path.\")\n",
    "\n",
    "        self.pixel_pairs = []\n",
    "        for i in self.pixels[0]:\n",
    "            for j in self.pixels[1]:\n",
    "                self.pixel_pairs.append([i, j])\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Go file by file\n",
    "        for file in files:\n",
    "            print(f\"Processing file: {file}\")\n",
    "\n",
    "            start_time_unpack = time.time()\n",
    "\n",
    "            # Unpack the data from the file\n",
    "            data = self._unpack_binary_data(file)\n",
    "\n",
    "            # Pre-collect the indices of the acquisition cycles' ends\n",
    "            self.cycle_ends = np.argwhere(data[0].T[0] == -2)\n",
    "            self.cycle_ends = np.insert(self.cycle_ends, 0, 0)\n",
    "\n",
    "            start_time_args = time.time()\n",
    "\n",
    "            # Prepare the arguments for the child processes:\n",
    "            # 'file' and 'pixel_pair' for naming purposes,\n",
    "            # chunked data as a tuple of two arrays to work with\n",
    "            args = [\n",
    "                (\n",
    "                    file,\n",
    "                    self._chunk_that_data(data, pixel_pair),\n",
    "                    pixel_pair,\n",
    "                )\n",
    "                for pixel_pair in self.pixel_pairs\n",
    "            ]\n",
    "\n",
    "            # print(f\"Created args in {time.time() - start_time_args}\")\n",
    "            print(\"Before the processes\", time.time())\n",
    "\n",
    "            with multiprocessing.Pool(\n",
    "                min(self.number_of_cores, len(self.pixel_pairs))\n",
    "            ) as pool:\n",
    "                pool.map(\n",
    "                    self._calculate_timestamps_differences,\n",
    "                    args,\n",
    "                )\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(\n",
    "            f\"Parallel processing of files \"\n",
    "            \"files (with each writing to its file) finished \"\n",
    "            f\"in: {round(end_time - start_time, 2)} s\"\n",
    "        )\n",
    "\n",
    "        # Combine '.feather' files from separate cores\n",
    "        path_to_feathers = os.path.join(self.path, \"delta_ts_data\")\n",
    "\n",
    "        self._combine_feather_files(path_to_feathers)\n",
    "\n",
    "        path_output = os.path.join(self.path, \"delta_ts_data\")\n",
    "\n",
    "        print(\n",
    "            \"The feather files with the timestamp differences were \"\n",
    "            f\"combined into the 'combined.feather' file in {path_output}\"\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # path = r\"D:\\LinoSPAD2\\Data\\B7d\\2024.11.11\\500t\\MP_test\"\n",
    "    path = r'/media/sj/King4TB/LS2_Data/2024.11.11/500t/MP_test_70'\n",
    "\n",
    "    mp = MpWizard(\n",
    "        path,\n",
    "        # pixels=[[144], [171, 172]],\n",
    "        pixels=[[x for x in range(55, 60)], [x for x in range(175, 180)]],\n",
    "        # pixels=[[x for x in range(20, 80)], [x for x in range(130, 190)]],\n",
    "        daughterboard_number=\"B7d\",\n",
    "        motherboard_number=\"#28\",\n",
    "        firmware_version=\"2212b\",\n",
    "        timestamps=500,\n",
    "        number_of_cores=5,\n",
    "    )\n",
    "\n",
    "    mp.calculate_and_save_timestamp_differences_mp()\n",
    "\n",
    "\n",
    "### Standard approach - for control and comparison\n",
    "\n",
    "import time\n",
    "\n",
    "from daplis.functions import delta_t\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "path = r\"/media/sj/King4TB/LS2_Data/2024.11.11/500t/MP_test_70\"\n",
    "\n",
    "\n",
    "delta_t.calculate_and_save_timestamp_differences_fast(\n",
    "    path,\n",
    "    rewrite=True,\n",
    "    # pixels=[144, 171],\n",
    "    pixels=[[x for x in range(55, 60)], [x for x in range(175, 180)]],\n",
    "    # pixels=[[x for x in range(20, 80)], [x for x in range(130, 190)]],\n",
    "    daughterboard_number=\"B7d\",\n",
    "    motherboard_number=\"#28\",\n",
    "    firmware_version=\"2212b\",\n",
    "    timestamps=500,\n",
    ")\n",
    "\n",
    "print(f\"Finished in {time.time() - time_start}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daplis_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
